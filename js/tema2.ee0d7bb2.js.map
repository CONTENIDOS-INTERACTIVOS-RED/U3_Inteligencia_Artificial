{"version":3,"sources":["webpack:///./src/assets/curso/tema2/24.png","webpack:///./src/assets/template/icono-yt.svg","webpack:///./src/assets/curso/tema2/22.svg","webpack:///./src/assets/curso/tema2/25.png","webpack:///./src/assets/curso/tema2/10.png","webpack:///./src/assets/curso/tema2/slide-2.png","webpack:///./src/assets/curso/podcast/podcast1.mp3","webpack:///./src/assets/curso/tema2/19.svg","webpack:///./src/assets/componentes/material-complementario.svg","webpack:///./src/assets/curso/tema2/17.svg","webpack:///./src/assets/curso/tema2/18.svg","webpack:///./src/assets/curso/tema2/23.png","webpack:///./src/assets/curso/tema2/16.svg","webpack:///./src/assets/curso/tema2/slide-4.png","webpack:///./src/views/curso/Tema2.vue?4c6b","webpack:///src/views/curso/Tema2.vue","webpack:///./src/views/curso/Tema2.vue?efc3","webpack:///./src/views/curso/Tema2.vue","webpack:///./src/assets/curso/tema2/1.png","webpack:///./src/assets/curso/tema2/15.svg","webpack:///./src/assets/curso/tema2/slide-3.png","webpack:///./src/assets/curso/tema2/info1.png","webpack:///./src/assets/curso/tema2/5.svg","webpack:///./src/assets/curso/tema2/7.png","webpack:///./src/assets/curso/tema2/6.png","webpack:///./src/assets/curso/tema2/11.png","webpack:///./src/assets/curso/tema2/13.svg","webpack:///./src/assets/componentes/link.svg","webpack:///./src/assets/curso/tema2/8.svg","webpack:///./src/assets/curso/tema2/20.svg","webpack:///./src/assets/curso/tema2/2.png","webpack:///./src/assets/curso/tema2/slide-1.png","webpack:///./src/assets/curso/tema2/14.png","webpack:///./src/assets/curso/tema2/26.png","webpack:///./src/assets/curso/tema2/9.png","webpack:///./src/assets/curso/tema2/4.svg","webpack:///./src/assets/curso/tema2/21.svg","webpack:///./src/assets/template/icono-pdf.svg"],"names":["module","exports","render","_vm","this","_h","$createElement","_c","_self","staticClass","attrs","_v","obtenerLink","style","_m","scopedSlots","_u","key","fn","staticStyle","proxy","staticRenderFns","name","mounted","$nextTick","component"],"mappings":"4FAAAA,EAAOC,QAAU,IAA0B,uB,uBCA3CD,EAAOC,QAAU,IAA0B,6B,uBCA3CD,EAAOC,QAAU,IAA0B,uB,uBCA3CD,EAAOC,QAAU,IAA0B,uB,uBCA3CD,EAAOC,QAAU,IAA0B,uB,uBCA3CD,EAAOC,QAAU,IAA0B,4B,qBCA3CD,EAAOC,QAAU,IAA0B,+B,qBCA3CD,EAAOC,QAAU,IAA0B,uB,qBCA3CD,EAAOC,QAAU,IAA0B,4C,uBCA3CD,EAAOC,QAAU,IAA0B,uB,uBCA3CD,EAAOC,QAAU,IAA0B,uB,uBCA3CD,EAAOC,QAAU,IAA0B,uB,uBCA3CD,EAAOC,QAAU,IAA0B,uB,uBCA3CD,EAAOC,QAAU,IAA0B,4B,2CCA3C,IAAIC,EAAS,WAAa,IAAIC,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,gBAAgB,CAACG,MAAM,CAAC,UAAY,sEAAsEH,EAAG,MAAM,CAACE,YAAY,gEAAgE,CAACF,EAAG,IAAI,CAACJ,EAAIQ,GAAG,iLAAiLJ,EAAG,MAAM,CAACE,YAAY,oCAAoC,CAACF,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,KAAK,CAACE,YAAY,kBAAkBC,MAAM,CAAC,WAAW,YAAY,CAACP,EAAIQ,GAAG,mEAAmEJ,EAAG,IAAI,CAACE,YAAY,OAAOC,MAAM,CAAC,WAAW,eAAe,CAACP,EAAIQ,GAAG,kXAAkXJ,EAAG,IAAI,CAACE,YAAY,4BAA4BC,MAAM,CAAC,KAAOP,EAAIS,YAAY,0BAA0B,OAAS,SAAS,WAAW,YAAY,CAACL,EAAG,MAAM,CAACE,YAAY,eAAeI,MAAM,CAAE,mBAAoB,YAAa,CAACN,EAAG,MAAM,CAACG,MAAM,CAAC,IAAM,EAAQ,aAAwCP,EAAIW,GAAG,OAAOX,EAAIW,GAAG,SAASX,EAAIW,GAAG,GAAGP,EAAG,IAAI,CAACJ,EAAIQ,GAAG,+kBAA+kBJ,EAAG,MAAM,CAACE,YAAY,oCAAoC,CAACF,EAAG,MAAM,CAACE,YAAY,cAAc,CAACF,EAAG,MAAM,CAACE,YAAY,sDAAsD,CAACF,EAAG,MAAM,CAACE,YAAY,aAAa,CAACF,EAAG,oBAAoB,CAACE,YAAY,mBAAmBM,YAAYZ,EAAIa,GAAG,CAAC,CAACC,IAAI,SAASC,GAAG,WAAW,MAAO,CAACX,EAAG,SAAS,CAACA,EAAG,MAAM,CAACE,YAAY,UAAUU,YAAY,CAAC,YAAY,SAAST,MAAM,CAAC,IAAM,EAAQ,QAAkC,IAAM,UAAUU,OAAM,MAAS,CAACb,EAAG,MAAM,CAACE,YAAY,gCAAgCC,MAAM,CAAC,EAAI,MAAM,EAAI,MAAM,OAAS,MAAM,CAACH,EAAG,KAAK,CAACJ,EAAIQ,GAAG,uBAAuBJ,EAAG,IAAI,CAACJ,EAAIQ,GAAG,8FAA8FJ,EAAG,MAAM,CAACE,YAAY,gCAAgCC,MAAM,CAAC,EAAI,MAAM,EAAI,MAAM,OAAS,MAAM,CAACH,EAAG,KAAK,CAACJ,EAAIQ,GAAG,wBAAwBJ,EAAG,IAAI,CAACJ,EAAIQ,GAAG,2FAA2FJ,EAAG,MAAM,CAACE,YAAY,gCAAgCC,MAAM,CAAC,EAAI,MAAM,EAAI,MAAM,OAAS,MAAM,CAACH,EAAG,KAAK,CAACJ,EAAIQ,GAAG,8BAA8BJ,EAAG,IAAI,CAACJ,EAAIQ,GAAG,gHAAgH,SAASR,EAAIW,GAAG,GAAGP,EAAG,IAAI,CAACJ,EAAIQ,GAAG,8MAA8MR,EAAIW,GAAG,GAAGX,EAAIW,GAAG,GAAGP,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,cAAc,CAACN,EAAIW,GAAG,GAAGP,EAAG,MAAM,CAACE,YAAY,iDAAiD,CAACF,EAAG,MAAM,CAACE,YAAY,UAAU,CAACN,EAAIW,GAAG,GAAGP,EAAG,eAAe,CAACE,YAAY,2CAA2CC,MAAM,CAAC,MAAQ,2BAA2B,OAAS,GAAG,MAAQ,EAAQ,YAA+C,GAAGP,EAAIW,GAAG,SAASX,EAAIW,GAAG,GAAGP,EAAG,IAAI,CAACJ,EAAIQ,GAAG,6HAA6HJ,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACN,EAAIW,GAAG,IAAIP,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,YAAY,CAACG,MAAM,CAAC,KAAO,IAAI,GAAK,cAAc,CAACH,EAAG,MAAM,CAACE,YAAY,MAAMC,MAAM,CAAC,OAAS,+BAA+B,CAACH,EAAG,IAAI,CAACJ,EAAIQ,GAAG,oOAAoOJ,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,eAAeR,EAAIQ,GAAG,6hBAA6hBJ,EAAG,MAAM,CAACE,YAAY,MAAMC,MAAM,CAAC,OAAS,sCAAsC,CAACH,EAAG,IAAI,CAACJ,EAAIQ,GAAG,+OAA+OJ,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,eAAeR,EAAIQ,GAAG,2dAA2dJ,EAAG,MAAM,CAACE,YAAY,MAAMC,MAAM,CAAC,OAAS,kDAAkD,CAACH,EAAG,IAAI,CAACJ,EAAIQ,GAAG,0cAA0cJ,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,eAAeR,EAAIQ,GAAG,oPAAoP,KAAKJ,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,YAAY,CAACG,MAAM,CAAC,KAAO,IAAI,GAAK,cAAc,CAACH,EAAG,MAAM,CAACE,YAAY,MAAMC,MAAM,CAAC,OAAS,6BAA6B,CAACH,EAAG,IAAI,CAACJ,EAAIQ,GAAG,4ZAA8ZJ,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,eAAeR,EAAIQ,GAAG,wKAAwKJ,EAAG,MAAM,CAACE,YAAY,MAAMC,MAAM,CAAC,OAAS,mBAAmB,CAACH,EAAG,IAAI,CAACJ,EAAIQ,GAAG,iPAAiPJ,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,eAAeR,EAAIQ,GAAG,yKAAyKJ,EAAG,MAAM,CAACE,YAAY,MAAMC,MAAM,CAAC,OAAS,0BAA0B,CAACH,EAAG,IAAI,CAACJ,EAAIQ,GAAG,8RAA8RJ,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,eAAeR,EAAIQ,GAAG,+PAA+P,GAAGR,EAAIW,GAAG,MAAMP,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACN,EAAIW,GAAG,IAAIP,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,YAAY,CAACG,MAAM,CAAC,KAAO,IAAI,GAAK,cAAc,CAACH,EAAG,MAAM,CAACE,YAAY,MAAMC,MAAM,CAAC,OAAS,4BAA4B,CAACH,EAAG,IAAI,CAACJ,EAAIQ,GAAG,mMAAmMJ,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,eAAeR,EAAIQ,GAAG,yJAA2JJ,EAAG,MAAM,CAACE,YAAY,MAAMC,MAAM,CAAC,OAAS,wBAAwB,CAACH,EAAG,IAAI,CAACJ,EAAIQ,GAAG,iNAAiNJ,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,eAAeR,EAAIQ,GAAG,mIAAmIJ,EAAG,MAAM,CAACE,YAAY,MAAMC,MAAM,CAAC,OAAS,yBAAyB,CAACH,EAAG,IAAI,CAACJ,EAAIQ,GAAG,mLAAmLJ,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,eAAeR,EAAIQ,GAAG,qLAAqL,KAAKR,EAAIW,GAAG,IAAIX,EAAIW,GAAG,IAAIX,EAAIW,GAAG,IAAIX,EAAIW,GAAG,IAAIX,EAAIW,GAAG,IAAIX,EAAIW,GAAG,IAAIP,EAAG,KAAK,CAACJ,EAAIQ,GAAG,kDAAkDJ,EAAG,IAAI,CAACJ,EAAIQ,GAAG,oKAAoKJ,EAAG,MAAM,CAACE,YAAY,sCAAsC,CAACF,EAAG,MAAM,CAACE,YAAY,cAAc,CAACF,EAAG,UAAU,CAACE,YAAY,WAAWC,MAAM,CAAC,KAAO,MAAM,CAACH,EAAG,MAAM,CAACE,YAAY,qCAAqC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,SAAS,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,IAAM,EAAQ,eAA2CH,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,KAAK,CAACJ,EAAIQ,GAAG,yBAAyBJ,EAAG,IAAI,CAACJ,EAAIQ,GAAG,qCAAqCJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,gBAAgBR,EAAIQ,GAAG,KAAKJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,2BAA2BR,EAAIQ,GAAG,KAAKJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,mBAAmBR,EAAIQ,GAAG,MAAMJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,oBAAoBR,EAAIQ,GAAG,gDAAgDJ,EAAG,MAAM,CAACE,YAAY,qCAAqC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,SAAS,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,IAAM,EAAQ,eAA2CH,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,KAAK,CAACJ,EAAIQ,GAAG,4CAA4CJ,EAAG,IAAI,CAACJ,EAAIQ,GAAG,sFAAsFJ,EAAG,MAAM,CAACE,YAAY,qCAAqC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,SAAS,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,IAAM,EAAQ,eAA2CH,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,KAAK,CAACJ,EAAIQ,GAAG,oCAAoCJ,EAAG,IAAI,CAACJ,EAAIQ,GAAG,sBAAsBJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,oBAAoBR,EAAIQ,GAAG,KAAKJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,wBAAwBR,EAAIQ,GAAG,MAAMJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,sBAAsBR,EAAIQ,GAAG,kDAAkDJ,EAAG,MAAM,CAACE,YAAY,qCAAqC,CAACF,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,SAAS,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,IAAM,EAAQ,eAA2CH,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,KAAK,CAACJ,EAAIQ,GAAG,4BAA4BJ,EAAG,IAAI,CAACJ,EAAIQ,GAAG,4GAA4G,KAAKJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,iCAAiCJ,EAAG,IAAI,CAACJ,EAAIQ,GAAG,6MAA6MR,EAAIW,GAAG,IAAIX,EAAIW,GAAG,IAAIX,EAAIW,GAAG,IAAIP,EAAG,MAAM,CAACE,YAAY,2CAA2C,CAACF,EAAG,MAAM,CAACE,YAAY,cAAc,CAACF,EAAG,KAAK,CAACG,MAAM,CAAC,WAAW,cAAc,CAACP,EAAIQ,GAAG,6BAA6BJ,EAAG,MAAM,CAACE,YAAY,+BAA+B,CAACF,EAAG,MAAM,CAACE,YAAY,4BAA4B,CAACF,EAAG,IAAI,CAACJ,EAAIQ,GAAG,4MAA4MJ,EAAG,IAAI,CAACE,YAAY,eAAe,CAACF,EAAG,MAAM,CAACE,YAAY,OAAOI,MAAM,CAAE,YAAY,QAASH,MAAM,CAAC,IAAM,EAAQ,WAAoCH,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,gEAAgE,OAAS,SAAS,IAAM,wBAAwB,CAACP,EAAIQ,GAAG,0IAA0IJ,EAAG,IAAI,CAACE,YAAY,eAAe,CAACF,EAAG,MAAM,CAACE,YAAY,OAAOI,MAAM,CAAE,YAAY,QAASH,MAAM,CAAC,IAAM,EAAQ,WAAqCH,EAAG,IAAI,CAACG,MAAM,CAAC,KAAO,8CAA8C,OAAS,SAAS,IAAM,wBAAwB,CAACP,EAAIQ,GAAG,gFAAgFR,EAAIW,GAAG,aAAa,IACt5cO,EAAkB,CAAC,WAAa,IAAIlB,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,gBAAgB,CAACF,EAAG,IAAI,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,aAAaR,EAAIQ,GAAG,kCAAkC,WAAa,IAAIR,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,SAAS,CAACG,MAAM,CAAC,WAAW,YAAY,CAACH,EAAG,MAAM,CAACG,MAAM,CAAC,IAAM,EAAQ,QAA8B,IAAM,WAAW,WAAa,IAAIP,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,QAAQ,WAAW,YAAY,CAACH,EAAG,KAAK,CAACA,EAAG,OAAO,CAACJ,EAAIQ,GAAG,SAASR,EAAIQ,GAAG,2BAA2B,WAAa,IAAIR,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,SAAS,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,IAAM,EAAQ,QAA8B,WAAW,iBAAiBH,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,IAAI,CAACE,YAAY,OAAOC,MAAM,CAAC,WAAW,cAAc,CAACP,EAAIQ,GAAG,2QAA2QJ,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,WAAW,cAAc,CAACH,EAAG,IAAI,CAACE,YAAY,QAAQ,CAACN,EAAIQ,GAAG,ogBAAogB,WAAa,IAAIR,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,sBAAsBC,MAAM,CAAC,WAAW,cAAc,CAACH,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,MAAM,CAACE,YAAY,UAAUU,YAAY,CAAC,YAAY,QAAQT,MAAM,CAAC,IAAM,EAAQ,aAAmCH,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACE,YAAY,QAAQ,CAACN,EAAIQ,GAAG,mSAAmSJ,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,MAAM,CAACE,YAAY,UAAUU,YAAY,CAAC,YAAY,QAAQT,MAAM,CAAC,IAAM,EAAQ,aAAmCH,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACE,YAAY,QAAQ,CAACN,EAAIQ,GAAG,gOAAgOJ,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,SAAS,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,IAAM,EAAQ,QAA8B,WAAW,oBAAoB,WAAa,IAAIP,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,SAAS,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,IAAM,EAAQ,QAA8B,WAAW,iBAAiBH,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,IAAI,CAACE,YAAY,OAAOC,MAAM,CAAC,WAAW,cAAc,CAACP,EAAIQ,GAAG,ioBAAioBJ,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,WAAW,cAAc,CAACH,EAAG,IAAI,CAACE,YAAY,QAAQ,CAACN,EAAIQ,GAAG,2MAA2M,WAAa,IAAIR,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,QAAQ,WAAW,YAAY,CAACH,EAAG,KAAK,CAACA,EAAG,OAAO,CAACJ,EAAIQ,GAAG,SAASR,EAAIQ,GAAG,kCAAkC,WAAa,IAAIR,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,IAAI,CAACE,YAAY,OAAOC,MAAM,CAAC,WAAW,cAAc,CAACP,EAAIQ,GAAG,2EAA2EJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,aAAaR,EAAIQ,GAAG,QAAQ,WAAa,IAAIR,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,SAAS,CAACA,EAAG,MAAM,CAACY,YAAY,CAAC,YAAY,SAAST,MAAM,CAAC,IAAM,EAAQ,QAA8B,WAAW,kBAAkB,WAAa,IAAIP,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,QAAQ,WAAW,YAAY,CAACH,EAAG,KAAK,CAACA,EAAG,OAAO,CAACJ,EAAIQ,GAAG,SAASR,EAAIQ,GAAG,wBAAwB,WAAa,IAAIR,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,SAAS,CAACA,EAAG,MAAM,CAACE,YAAY,eAAeC,MAAM,CAAC,IAAM,EAAQ,QAA8B,IAAM,WAAW,WAAa,IAAIP,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,SAAS,CAACA,EAAG,MAAM,CAACE,YAAY,eAAeC,MAAM,CAAC,IAAM,EAAQ,QAA+B,IAAM,WAAW,WAAa,IAAIP,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,SAAS,CAACA,EAAG,MAAM,CAACE,YAAY,eAAeC,MAAM,CAAC,IAAM,EAAQ,QAA+B,IAAM,WAAW,WAAa,IAAIP,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,qBAAqB,CAACF,EAAG,MAAM,CAACE,YAAY,0BAA0B,CAACF,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,MAAM,CAACE,YAAY,UAAUU,YAAY,CAAC,YAAY,QAAQT,MAAM,CAAC,IAAM,EAAQ,aAAoCH,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,IAAI,CAACE,YAAY,QAAQ,CAACN,EAAIQ,GAAG,0KAA0K,WAAa,IAAIR,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,sBAAsB,CAACF,EAAG,KAAK,CAACJ,EAAIQ,GAAG,eAAeJ,EAAG,OAAO,CAACJ,EAAIQ,GAAG,kDAAkD,WAAa,IAAIR,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,uCAAuC,CAACF,EAAG,QAAQ,CAACE,YAAY,SAASC,MAAM,CAAC,GAAK,WAAW,CAACH,EAAG,QAAQ,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACE,YAAY,6BAA6B,CAACN,EAAIQ,GAAG,sBAAsBJ,EAAG,KAAK,CAACE,YAAY,6BAA6B,CAACN,EAAIQ,GAAG,iBAAiBJ,EAAG,KAAK,CAACE,YAAY,6BAA6B,CAACN,EAAIQ,GAAG,sBAAsBJ,EAAG,KAAK,CAACE,YAAY,6BAA6B,CAACN,EAAIQ,GAAG,uBAAuBJ,EAAG,QAAQ,CAACA,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,uBAAuBJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,sEAAsEJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,4BAA4BJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,0DAA0DJ,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,oBAAoBJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,0DAA0DJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,oCAAoCJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,qDAAqDJ,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,mBAAmBJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,4DAA4DJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,kCAAkCJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,2CAA2CJ,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,aAAaJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,mCAAmCJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,uCAAuCJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,wCAAwCJ,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,oBAAoBJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,sDAAsDJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,4BAA4BJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,0CAA0CJ,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,sBAAsBJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,kCAAkCJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,iCAAiCJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,gCAAgCJ,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,sBAAsBJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,mDAAmDJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,6BAA6BJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,iCAAiCJ,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,iBAAiBJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,mDAAmDJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,wBAAwBJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,iCAAiCJ,EAAG,KAAK,CAACA,EAAG,KAAK,CAACA,EAAG,SAAS,CAACJ,EAAIQ,GAAG,mBAAmBJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,qEAAqEJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,0BAA0BJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,+CAA+C,WAAa,IAAIR,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,iBAAiBC,MAAM,CAAC,GAAK,QAAQ,WAAW,YAAY,CAACH,EAAG,KAAK,CAACA,EAAG,OAAO,CAACJ,EAAIQ,GAAG,SAASR,EAAIQ,GAAG,6CAA6C,WAAa,IAAIR,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,4BAA4BC,MAAM,CAAC,WAAW,cAAc,CAACH,EAAG,IAAI,CAACJ,EAAIQ,GAAG,mSAAmSJ,EAAG,IAAI,CAACE,YAAY,QAAQ,CAACN,EAAIQ,GAAG,gJAAgJJ,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,SAAS,CAACA,EAAG,MAAM,CAACE,YAAY,eAAeC,MAAM,CAAC,IAAM,EAAQ,QAA+B,WAAW,oBAAoB,WAAa,IAAIP,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,iCAAiC,CAACF,EAAG,MAAM,CAACE,YAAY,cAAc,CAACF,EAAG,KAAK,CAACJ,EAAIQ,GAAG,qCAAqCJ,EAAG,IAAI,CAACJ,EAAIQ,GAAG,yIAAyIJ,EAAG,MAAM,CAACE,YAAY,uDAAuD,CAACF,EAAG,MAAM,CAACE,YAAY,gBAAgBC,MAAM,CAAC,WAAW,eAAe,CAACH,EAAG,MAAM,CAACE,YAAY,6CAA6C,CAACF,EAAG,MAAM,CAACE,YAAY,uBAAuBU,YAAY,CAAC,MAAQ,QAAQT,MAAM,CAAC,IAAM,EAAQ,QAA+B,IAAM,MAAMH,EAAG,KAAK,CAACE,YAAY,eAAe,CAACN,EAAIQ,GAAG,wBAAwBJ,EAAG,IAAI,CAACE,YAAY,oBAAoB,CAACN,EAAIQ,GAAG,6FAA6FJ,EAAG,MAAM,CAACE,YAAY,gBAAgBC,MAAM,CAAC,WAAW,eAAe,CAACH,EAAG,MAAM,CAACE,YAAY,6CAA6C,CAACF,EAAG,MAAM,CAACE,YAAY,uBAAuBU,YAAY,CAAC,MAAQ,QAAQT,MAAM,CAAC,IAAM,EAAQ,QAA+B,IAAM,MAAMH,EAAG,KAAK,CAACE,YAAY,eAAe,CAACN,EAAIQ,GAAG,gCAAgCJ,EAAG,IAAI,CAACE,YAAY,oBAAoB,CAACN,EAAIQ,GAAG,gGAAgGJ,EAAG,MAAM,CAACE,YAAY,gBAAgBC,MAAM,CAAC,WAAW,eAAe,CAACH,EAAG,MAAM,CAACE,YAAY,6CAA6C,CAACF,EAAG,MAAM,CAACE,YAAY,uBAAuBU,YAAY,CAAC,MAAQ,QAAQT,MAAM,CAAC,IAAM,EAAQ,QAA+B,IAAM,MAAMH,EAAG,KAAK,CAACE,YAAY,eAAe,CAACN,EAAIQ,GAAG,yBAAyBJ,EAAG,IAAI,CAACE,YAAY,oBAAoB,CAACN,EAAIQ,GAAG,4EAA4EJ,EAAG,MAAM,CAACE,YAAY,gBAAgBC,MAAM,CAAC,WAAW,eAAe,CAACH,EAAG,MAAM,CAACE,YAAY,6CAA6C,CAACF,EAAG,MAAM,CAACE,YAAY,uBAAuBU,YAAY,CAAC,MAAQ,QAAQT,MAAM,CAAC,IAAM,EAAQ,QAA+B,IAAM,MAAMH,EAAG,KAAK,CAACE,YAAY,eAAe,CAACN,EAAIQ,GAAG,sBAAsBJ,EAAG,IAAI,CAACE,YAAY,oBAAoB,CAACN,EAAIQ,GAAG,6GAA6G,WAAa,IAAIR,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,uDAAuD,CAACF,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,uBAAuBC,MAAM,CAAC,WAAW,cAAc,CAACH,EAAG,MAAM,CAACE,YAAY,0BAA0B,CAACF,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,MAAM,CAACE,YAAY,UAAUU,YAAY,CAAC,YAAY,QAAQT,MAAM,CAAC,IAAM,EAAQ,aAAoCH,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACE,YAAY,QAAQ,CAACN,EAAIQ,GAAG,yBAAyBJ,EAAG,IAAI,CAACE,YAAY,QAAQ,CAACN,EAAIQ,GAAG,6FAA6FJ,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,uBAAuBC,MAAM,CAAC,WAAW,cAAc,CAACH,EAAG,MAAM,CAACE,YAAY,0BAA0B,CAACF,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,MAAM,CAACE,YAAY,UAAUU,YAAY,CAAC,YAAY,QAAQT,MAAM,CAAC,IAAM,EAAQ,aAAoCH,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACE,YAAY,QAAQ,CAACF,EAAG,KAAK,CAACJ,EAAIQ,GAAG,cAAcR,EAAIQ,GAAG,aAAaJ,EAAG,IAAI,CAACE,YAAY,QAAQ,CAACN,EAAIQ,GAAG,+HAA+H,WAAa,IAAIR,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,uDAAuD,CAACF,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,uBAAuBC,MAAM,CAAC,WAAW,cAAc,CAACH,EAAG,MAAM,CAACE,YAAY,0BAA0B,CAACF,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,MAAM,CAACE,YAAY,UAAUU,YAAY,CAAC,YAAY,QAAQT,MAAM,CAAC,IAAM,EAAQ,aAAoCH,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACE,YAAY,QAAQ,CAACN,EAAIQ,GAAG,0BAA0BJ,EAAG,IAAI,CAACE,YAAY,QAAQ,CAACN,EAAIQ,GAAG,kFAAkFJ,EAAG,MAAM,CAACE,YAAY,YAAY,CAACF,EAAG,MAAM,CAACE,YAAY,uBAAuBC,MAAM,CAAC,WAAW,cAAc,CAACH,EAAG,MAAM,CAACE,YAAY,0BAA0B,CAACF,EAAG,MAAM,CAACE,YAAY,eAAe,CAACF,EAAG,MAAM,CAACE,YAAY,UAAUU,YAAY,CAAC,YAAY,QAAQT,MAAM,CAAC,IAAM,EAAQ,aAAoCH,EAAG,MAAM,CAACE,YAAY,UAAU,CAACF,EAAG,KAAK,CAACE,YAAY,QAAQ,CAACN,EAAIQ,GAAG,iCAAiCJ,EAAG,IAAI,CAACE,YAAY,QAAQ,CAACN,EAAIQ,GAAG,4HAA4H,WAAa,IAAIR,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,8BAA8B,CAACF,EAAG,MAAM,CAACE,YAAY,qBAAqB,CAACF,EAAG,KAAK,CAACJ,EAAIQ,GAAG,oCAAoCJ,EAAG,IAAI,CAACJ,EAAIQ,GAAG,0JAA0JJ,EAAG,MAAM,CAACE,YAAY,uDAAuD,CAACF,EAAG,MAAM,CAACE,YAAY,gBAAgBC,MAAM,CAAC,WAAW,eAAe,CAACH,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,2BAA2BC,MAAM,CAAC,IAAM,EAAQ,QAA+B,IAAM,MAAMH,EAAG,MAAM,CAACE,YAAY,+BAA+B,CAACF,EAAG,KAAK,CAACE,YAAY,oBAAoB,CAACN,EAAIQ,GAAG,sBAAsBJ,EAAG,IAAI,CAACE,YAAY,oBAAoB,CAACN,EAAIQ,GAAG,6IAA6IJ,EAAG,MAAM,CAACE,YAAY,gBAAgBC,MAAM,CAAC,WAAW,iBAAiB,CAACH,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,2BAA2BC,MAAM,CAAC,IAAM,EAAQ,QAA+B,IAAM,MAAMH,EAAG,MAAM,CAACE,YAAY,+BAA+B,CAACF,EAAG,KAAK,CAACE,YAAY,oBAAoB,CAACN,EAAIQ,GAAG,sBAAsBJ,EAAG,IAAI,CAACE,YAAY,oBAAoB,CAACN,EAAIQ,GAAG,iHAAiHJ,EAAG,MAAM,CAACE,YAAY,gBAAgBC,MAAM,CAAC,WAAW,iBAAiB,CAACH,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,2BAA2BC,MAAM,CAAC,IAAM,EAAQ,QAA+B,IAAM,MAAMH,EAAG,MAAM,CAACE,YAAY,+BAA+B,CAACF,EAAG,KAAK,CAACE,YAAY,oBAAoB,CAACN,EAAIQ,GAAG,mCAAmCJ,EAAG,IAAI,CAACE,YAAY,oBAAoB,CAACN,EAAIQ,GAAG,2BAA2BJ,EAAG,KAAK,CAACJ,EAAIQ,GAAG,2CAA2CR,EAAIQ,GAAG,oFAAoFJ,EAAG,MAAM,CAACE,YAAY,gBAAgBC,MAAM,CAAC,WAAW,iBAAiB,CAACH,EAAG,MAAM,CAACE,YAAY,6BAA6B,CAACF,EAAG,MAAM,CAACE,YAAY,2BAA2BC,MAAM,CAAC,IAAM,EAAQ,QAA+B,IAAM,MAAMH,EAAG,MAAM,CAACE,YAAY,+BAA+B,CAACF,EAAG,KAAK,CAACE,YAAY,oBAAoB,CAACN,EAAIQ,GAAG,wBAAwBJ,EAAG,IAAI,CAACE,YAAY,oBAAoB,CAACN,EAAIQ,GAAG,uJAAuJ,WAAa,IAAIR,EAAIC,KAASC,EAAGF,EAAIG,eAAmBC,EAAGJ,EAAIK,MAAMD,IAAIF,EAAG,OAAOE,EAAG,MAAM,CAACE,YAAY,wCAAwC,CAACF,EAAG,SAAS,CAACA,EAAG,MAAM,CAACG,MAAM,CAAC,IAAM,EAAQ,QAAoD,IAAM,YC8e7mlB,GACEY,KAAM,QACNC,QAFF,WAEA,WACInB,KAAKoB,WAAU,WACb,EAAN,mBCnf8V,I,YCO1VC,EAAY,eACd,EACAvB,EACAmB,GACA,EACA,KACA,KACA,MAIa,aAAAI,E,gCClBfzB,EAAOC,QAAU,IAA0B,sB,uBCA3CD,EAAOC,QAAU,IAA0B,uB,qBCA3CD,EAAOC,QAAU,IAA0B,4B,uBCA3CD,EAAOC,QAAU,IAA0B,0B,qBCA3CD,EAAOC,QAAU,IAA0B,sB,uBCA3CD,EAAOC,QAAU,IAA0B,sB,uBCA3CD,EAAOC,QAAU,IAA0B,sB,uBCA3CD,EAAOC,QAAU,IAA0B,uB,qBCA3CD,EAAOC,QAAU,IAA0B,uB,qBCA3CD,EAAOC,QAAU,IAA0B,yB,qBCA3CD,EAAOC,QAAU,IAA0B,sB,qBCA3CD,EAAOC,QAAU,IAA0B,uB,qBCA3CD,EAAOC,QAAU,IAA0B,sB,qBCA3CD,EAAOC,QAAU,IAA0B,4B,qBCA3CD,EAAOC,QAAU,IAA0B,uB,qBCA3CD,EAAOC,QAAU,IAA0B,uB,qBCA3CD,EAAOC,QAAU,IAA0B,sB,qBCA3CD,EAAOC,QAAU,IAA0B,sB,qBCA3CD,EAAOC,QAAU,IAA0B,uB,qBCA3CD,EAAOC,QAAU,IAA0B","file":"js/tema2.ee0d7bb2.js","sourcesContent":["module.exports = __webpack_public_path__ + \"img/24.1ed26bd8.png\";","module.exports = __webpack_public_path__ + \"img/icono-yt.0e5f1361.svg\";","module.exports = __webpack_public_path__ + \"img/22.897ac859.svg\";","module.exports = __webpack_public_path__ + \"img/25.7956582e.png\";","module.exports = __webpack_public_path__ + \"img/10.873918f0.png\";","module.exports = __webpack_public_path__ + \"img/slide-2.5b51275c.png\";","module.exports = __webpack_public_path__ + \"media/podcast1.fbd69f8a.mp3\";","module.exports = __webpack_public_path__ + \"img/19.803a1dbc.svg\";","module.exports = __webpack_public_path__ + \"img/material-complementario.06dbfac1.svg\";","module.exports = __webpack_public_path__ + \"img/17.f7022165.svg\";","module.exports = __webpack_public_path__ + \"img/18.05e49c1f.svg\";","module.exports = __webpack_public_path__ + \"img/23.6c15797c.png\";","module.exports = __webpack_public_path__ + \"img/16.eb876238.svg\";","module.exports = __webpack_public_path__ + \"img/slide-4.d6cd85e2.png\";","var render = function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"curso-main-container pb-3\"},[_c('BannerInterno',{attrs:{\"subTitulo\":'2. Sesgos en los algoritmos: cómo se generan y sus implicaciones'}}),_c('div',{staticClass:\"container tarjeta tarjeta--blanca p-4 p-md-5 overflow-hidden\"},[_c('p',[_vm._v(\"La presencia de sesgos en los algoritmos de inteligencia artificial representa un desafío crítico para garantizar la equidad, la transparencia y la ética en su aplicación.\")]),_c('div',{staticClass:\"bg-full-width bg-color-6 mb-lg-5\"},[_c('div',{staticClass:\"px-4 p-md-5\"},[_c('div',{staticClass:\"row justify-content-center align-items-center\"},[_c('div',{staticClass:\"col-lg-8\"},[_c('h2',{staticClass:\"mb-4 text-white\",attrs:{\"data-aos\":\"flip-up\"}},[_vm._v(\"Sesgos en los algoritmos: cómo se generan y sus implicaciones\")]),_c('p',{staticClass:\"mb-4\",attrs:{\"data-aos\":\"fade-right\"}},[_vm._v(\"En el PDF Sesgos en los algoritmos, se examinan las fuentes de estos sesgos, sus efectos en ámbitos clave como la contratación, la justicia y la salud, así como las estrategias para mitigarlos mediante enfoques técnicos, éticos y normativos. Este análisis proporciona herramientas fundamentales para promover un uso más justo y responsable de la IA en la sociedad.\")]),_c('a',{staticClass:\"anexo mb-4 bg-white w-fit\",attrs:{\"href\":_vm.obtenerLink('/downloads/Anexo_2.pdf'),\"target\":\"_blank\",\"data-aos\":\"flip-up\"}},[_c('div',{staticClass:\"anexo__icono\",style:({'background-color': '#FCDFDB'})},[_c('img',{attrs:{\"src\":require(\"@/assets/template/icono-pdf.svg\")}})]),_vm._m(0)])]),_vm._m(1)])])]),_vm._m(2),_c('p',[_vm._v(\"La transparencia y la explicabilidad son principios éticos claves que son esenciales para el desarrollo responsable de la IA. La disponibilidad a gran escala de datos de comportamiento humano y las mayores capacidades de la inteligencia artificial están permitiendo a investigadores, empresas, profesionales y responsables políticos co-desarrollar y evaluar en el mundo real procesos algorítmicos de toma de decisiones diseñados para maximizar la equidad, la rendición de cuentas y la transparencia, respetando al mismo tiempo la privacidad. A continuación, conozcamos los principales:\")]),_c('div',{staticClass:\"bg-full-width bg-color-info mb-5\"},[_c('div',{staticClass:\"p-4 p-md-5\"},[_c('div',{staticClass:\"row justify-content-center align-items-center mb-5\"},[_c('div',{staticClass:\"col-lg-10\"},[_c('ImagenInfografica',{staticClass:\"color-secundario\",scopedSlots:_vm._u([{key:\"imagen\",fn:function(){return [_c('figure',[_c('img',{staticClass:\"mx-auto\",staticStyle:{\"max-width\":\"873px\"},attrs:{\"src\":require(\"@/assets/curso/tema2/info1.png\"),\"alt\":\"\"}})])]},proxy:true}])},[_c('div',{staticClass:\"bg-color-white box-shadow p-3\",attrs:{\"x\":\"10%\",\"y\":\"64%\",\"numero\":\"+\"}},[_c('h5',[_vm._v(\"01. Transparencia\")]),_c('p',[_vm._v(\"Posibilidad de comprender cómo funcionan los algoritmos de IA y cómo toman decisiones.\")])]),_c('div',{staticClass:\"bg-color-white box-shadow p-3\",attrs:{\"x\":\"75%\",\"y\":\"23%\",\"numero\":\"+\"}},[_c('h5',[_vm._v(\"02. Explicabilidad\")]),_c('p',[_vm._v(\"Posibilidad de explicar las decisiones de la IA a las personas afectadas por ellas.\")])]),_c('div',{staticClass:\"bg-color-white box-shadow p-3\",attrs:{\"x\":\"90%\",\"y\":\"64%\",\"numero\":\"+\"}},[_c('h5',[_vm._v(\"03. Rendición de cuentas\")]),_c('p',[_vm._v(\"Necesidad de que haya individuos u organizaciones responsables de las acciones de los sistemas de IA.\")])])])],1)])])]),_vm._m(3),_c('p',[_vm._v(\"La rendición de cuentas puede demostrarse mediante una gestión eficaz, una supervisión periódica, la mitigación de riesgos, la evaluación del impacto, la comunicación abierta y la reparación de daños.\")]),_vm._m(4),_vm._m(5),_c('div',{staticClass:\"bg-full-width bg-fondo-2 mb-5\"},[_c('div',{staticClass:\"p-4 p-md-5\"},[_vm._m(6),_c('div',{staticClass:\"row justify-content-center align-items-center\"},[_c('div',{staticClass:\"col-lg\"},[_vm._m(7),_c('TarjetaAudio',{staticClass:\"color-acento-botones bg-color-white mb-3\",attrs:{\"texto\":\"Naturaleza de los sesgos\",\"tiempo\":\"\",\"audio\":require('../../assets/curso/podcast/podcast1.mp3')}})],1),_vm._m(8)])])]),_vm._m(9),_c('p',[_vm._v(\"Existen diferentes tipos de sesgo en los algoritmos de IA, a continuación, se tratarán de forma global los principales.\")]),_c('div',{staticClass:\"row align-items-start mb-5\"},[_vm._m(10),_c('div',{staticClass:\"col-lg-8\"},[_c('AcordionA',{attrs:{\"tipo\":\"b\",\"id\":\"Acordeon1\"}},[_c('div',{staticClass:\"div\",attrs:{\"titulo\":\"Sesgo de representatividad\"}},[_c('p',[_vm._v(\"Este fenómeno se produce cuando el sistema predictivo reduce la cantidad de datos utilizados por su naturaleza representativa, lo que puede influir negativamente en la validez estadístico-inductiva del modelo desarrollado.\")]),_c('p',[_c('strong',[_vm._v(\"Ejemplo. \")]),_vm._v(\"Podría ser el caso en el que se llevan a cabo devoluciones de productos en plataformas de comercio electrónico. Si se decide utilizar únicamente esta información para clasificar a un consumidor específico, se puede caer en la trampa de considerar a ese individuo como un cliente familiarizado. Esto ocurre debido a que el co-sistema en uso presenta un sesgo representativo que impacta negativamente en la línea de productos del sistema analizado, llevando a conclusiones que no reflejan la realidad general de todos los consumidores.\")])]),_c('div',{staticClass:\"div\",attrs:{\"titulo\":\"Sesgo de exterioridad (ambiental)\"}},[_c('p',[_vm._v(\"Esto ocurre cuando se excluyen ciertas características relevantes, lo que puede comprometer la validez de diversas relaciones que están presentes entre el ambiente que produce dicho sesgo y el efecto específico que se busca predecir.\")]),_c('p',[_c('strong',[_vm._v(\"Ejemplo. \")]),_vm._v(\"En un sistema predictivo de créditos, si no se incorporan adecuadamente los resultados de investigaciones económicas pertinentes, esto podría llevar a una mala asignación de las reputaciones crediticias de los usuarios. Esto no solo afecta a los individuos, sino que puede ocasionar repercusiones más amplias en el sistema financiero. Además, el sesgo puede derivar en decisiones que no reflejan la realidad económica y social que rodea a los solicitantes de crédito.\")])]),_c('div',{staticClass:\"div\",attrs:{\"titulo\":\"Sesgo de explicación (determinismo funcional)\"}},[_c('p',[_vm._v(\"Este fenómeno ocurre cuando dentro de los sistemas se excluyen grupos de individuos cuyos estímulos pueden ser similares o cuando, de alguna manera, se minimizan las predicciones realizadas por el sistema respecto a los resultados, lo que compromete de manera significativa la validez de las elecciones que realiza el usuario. Esto intenta incentivar para que esas elecciones se mantengan firmes y no se vean afectadas por las predicciones del sistema.\")]),_c('p',[_c('strong',[_vm._v(\"Ejemplo. \")]),_vm._v(\"Un ejemplo claro de esto es un sistema de entretenimiento cuyo software ha sido diseñado de tal manera que limita la exposición a ciertos contenidos, buscando influir en las preferencias del usuario y en sus decisiones subsiguientes.\")])])])],1)]),_c('div',{staticClass:\"row align-items-start mb-5\"},[_c('div',{staticClass:\"col-lg-8\"},[_c('AcordionA',{attrs:{\"tipo\":\"b\",\"id\":\"Acordeon2\"}},[_c('div',{staticClass:\"div\",attrs:{\"titulo\":\"Sesgo de responsabilidad\"}},[_c('p',[_vm._v(\"Este sesgo surge cuando no está claro quién debe asumir la responsabilidad por los resultados de un sistema de IA, especialmente cuando esos resultados son perjudiciales. La Inteligencia Artificial, por su naturaleza, implica muchas capas de participación: diseñadores, ingenieros, empresas, usuarios, reguladores, etc. Esta multiplicidad de actores puede provocar una especie de \\\"zona gris\\\" legal y ética.\")]),_c('p',[_c('strong',[_vm._v(\"Ejemplo. \")]),_vm._v(\"Si un coche autónomo atropella a una persona, ¿quién tiene la culpa? ¿El fabricante del coche? ¿El programador del algoritmo de frenado? ¿El dueño del vehículo?\")])]),_c('div',{staticClass:\"div\",attrs:{\"titulo\":\"Sesgo de datos\"}},[_c('p',[_vm._v(\"Este es uno de los más comunes. Se produce cuando los datos usados para entrenar una IA son incompletos, desequilibrados o reflejan prejuicios existentes en la sociedad. El sistema aprende esos sesgos como si fueran verdades objetivas.\")]),_c('p',[_c('strong',[_vm._v(\"Ejemplo. \")]),_vm._v(\"Si un sistema de contratación se entrena con datos de empleados históricos, y estos reflejan una preferencia por hombres, el modelo aprenderá a preferir hombres.\")])]),_c('div',{staticClass:\"div\",attrs:{\"titulo\":\"Sesgo de confirmación\"}},[_c('p',[_vm._v(\"Este sesgo ocurre cuando el modelo (o las personas que lo usan) interpreta los resultados de la IA de forma que refuerza sus creencias previas, en lugar de cuestionarlas. A menudo, los sistemas son entrenados o ajustados para confirmar hipótesis iniciales en lugar de desafiarlas.\")]),_c('p',[_c('strong',[_vm._v(\"Ejemplo. \")]),_vm._v(\"Un sistema predictivo policial que detecta más crimen en barrios pobres simplemente porque históricamente se ha hecho más vigilancia ahí. El modelo refuerza esa suposición y sigue enviando más patrullas, aunque el crimen real no haya aumentado.\")])])])],1),_vm._m(11)]),_c('div',{staticClass:\"row align-items-start mb-5\"},[_vm._m(12),_c('div',{staticClass:\"col-lg-8\"},[_c('AcordionA',{attrs:{\"tipo\":\"b\",\"id\":\"Acordeon3\"}},[_c('div',{staticClass:\"div\",attrs:{\"titulo\":\"Sesgo de automatización\"}},[_c('p',[_vm._v(\"Este sesgo se refiere a la tendencia de los humanos a confiar demasiado en decisiones automatizadas, incluso cuando tienen dudas o cuando esas decisiones contradicen la lógica o los hechos.\")]),_c('p',[_c('strong',[_vm._v(\"Ejemplo. \")]),_vm._v(\"Un médico sigue la recomendación de una IA diagnóstica, aunque su experiencia le indique lo contrario, simplemente porque \\\"la IA debe saber más\\\".\")])]),_c('div',{staticClass:\"div\",attrs:{\"titulo\":\"Sesgo del algoritmo\"}},[_c('p',[_vm._v(\"Este sesgo aparece cuando las propias reglas, parámetros o arquitectura del algoritmo favorecen ciertos resultados. Incluso con buenos datos, un algoritmo mal diseñado puede producir decisiones injustas.\")]),_c('p',[_c('strong',[_vm._v(\"Ejemplo. \")]),_vm._v(\"Un sistema de búsqueda que prioriza resultados por número de clics, lo cual favorece información sensacionalista o sesgada.\")])]),_c('div',{staticClass:\"div\",attrs:{\"titulo\":\"Sesgo de observación\"}},[_c('p',[_vm._v(\"Este sesgo se presenta cuando la forma en que se recolectan los datos introduce distorsiones, es decir, cuando la observación misma afecta el fenómeno o los datos obtenidos.\")]),_c('p',[_c('strong',[_vm._v(\"Ejemplo. \")]),_vm._v(\"Un sistema de salud que se entrena solo con datos de pacientes de hospitales privados probablemente no refleje los problemas de salud de poblaciones rurales o sin acceso.\")])])])],1)]),_vm._m(13),_vm._m(14),_vm._m(15),_vm._m(16),_vm._m(17),_vm._m(18),_c('h2',[_vm._v(\"Durante el diseño y entrenamiento del modelo\")]),_c('p',[_vm._v(\"En esta etapa es donde se establecen los parámetros y estructuras que pueden amplificar sesgos, algunas estrategias para mitigar los sesgos en esta fase, son:\")]),_c('div',{staticClass:\"bg-full-width bg-fondo-slider mb-5\"},[_c('div',{staticClass:\"p-4 p-md-5\"},[_c('SlyderA',{staticClass:\"bg-white\",attrs:{\"tipo\":\"b\"}},[_c('div',{staticClass:\"row align-items-center p-4 p-md-5\"},[_c('div',{staticClass:\"col-lg-5\"},[_c('figure',[_c('img',{attrs:{\"src\":require(\"@/assets/curso/tema2/slide-1.png\")}})])]),_c('div',{staticClass:\"col-lg-7\"},[_c('h4',[_vm._v(\"Equidad algorítmica\")]),_c('p',[_vm._v(\"Aplicar técnicas de fairness como\"),_c('em',[_vm._v(\"reweighing\")]),_vm._v(\",\"),_c('em',[_vm._v(\"adversarial debiasing\")]),_vm._v(\",\"),_c('em',[_vm._v(\"preprocessing\")]),_vm._v(\" y\"),_c('em',[_vm._v(\"postprocessing\")]),_vm._v(\" para corregir sesgos matemáticamente.\")])])]),_c('div',{staticClass:\"row align-items-center p-4 p-md-5\"},[_c('div',{staticClass:\"col-lg-5\"},[_c('figure',[_c('img',{attrs:{\"src\":require(\"@/assets/curso/tema2/slide-2.png\")}})])]),_c('div',{staticClass:\"col-lg-7\"},[_c('h4',[_vm._v(\"Regularización con métricas de equidad\")]),_c('p',[_vm._v(\"Incluir penalizaciones cuando el modelo trata injustamente a ciertos grupos.\")])])]),_c('div',{staticClass:\"row align-items-center p-4 p-md-5\"},[_c('div',{staticClass:\"col-lg-5\"},[_c('figure',[_c('img',{attrs:{\"src\":require(\"@/assets/curso/tema2/slide-3.png\")}})])]),_c('div',{staticClass:\"col-lg-7\"},[_c('h4',[_vm._v(\"Evaluación con métricas justas\")]),_c('p',[_vm._v(\"Usar métricas como\"),_c('em',[_vm._v(\"equalized odds\")]),_vm._v(\",\"),_c('em',[_vm._v(\"demographic parity\")]),_vm._v(\" o\"),_c('em',[_vm._v(\"disparate impact\")]),_vm._v(\", para medir discriminación algorítmica.\")])])]),_c('div',{staticClass:\"row align-items-center p-4 p-md-5\"},[_c('div',{staticClass:\"col-lg-5\"},[_c('figure',[_c('img',{attrs:{\"src\":require(\"@/assets/curso/tema2/slide-4.png\")}})])]),_c('div',{staticClass:\"col-lg-7\"},[_c('h4',[_vm._v(\"Modelos interpretables\")]),_c('p',[_vm._v(\"Favorecer modelos transparentes como árboles de decisión o regresiones, antes que cajas negras.\")])])])])],1)]),_c('h2',[_vm._v(\"Durante el despliegue y uso\")]),_c('p',[_vm._v(\"Un sistema puede funcionar bien durante el entrenamiento, pero fallar en la vida real si las condiciones cambian; por esto se plantean las siguientes estrategias, para ayudar a reducir estos riesgos:\")]),_vm._m(19),_vm._m(20),_vm._m(21),_c('div',{staticClass:\"bg-full-width border-top color-primario\"},[_c('div',{staticClass:\"p-4 p-md-5\"},[_c('h2',{attrs:{\"data-aos\":\"fade-left\"}},[_vm._v(\"MATERIAL COMPLEMENTARIO\")]),_c('div',{staticClass:\"row material-complementario\"},[_c('div',{staticClass:\"col-12 col-md-6 col-lg-7\"},[_c('p',[_vm._v(\"Los invitamos a explorar el material complementario de este curso, en esta sección encontrará recursos que le permitirán profundizar y enriquecer su aprendizaje en los temas tratados en esta unidad.\")]),_c('p',{staticClass:\"d-flex my-4\"},[_c('img',{staticClass:\"me-3\",style:({'max-width':'16px'}),attrs:{\"src\":require(\"@/assets/componentes/link.svg\")}}),_c('a',{attrs:{\"href\":\"https://elibro.net/es/lc/tecnologicadeloriente/titulos/117744\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\"}},[_vm._v(\"Casas Roma, J. Nin Guerrero, J. & Julbe López, F. (2019). Big data: análisis de datos en entornos masivos: ( ed.). Editorial UOC. \")])]),_c('p',{staticClass:\"d-flex my-4\"},[_c('img',{staticClass:\"me-3\",style:({'max-width':'16px'}),attrs:{\"src\":require(\"@/assets/template/icono-yt.svg\")}}),_c('a',{attrs:{\"href\":\"https://www.youtube.com/watch?v=ESLcIjf3Bs8\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\"}},[_vm._v(\"NEWMEDIA UFM. (2025, 20 de marzo). Perspectiva ética y social de la IA\")])])]),_vm._m(22)])])])])],1)}\nvar staticRenderFns = [function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"anexo__texto\"},[_c('p',[_c('strong',[_vm._v(\"Anexo. \")]),_vm._v(\" Sesgos en los algoritmos\")])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"col-lg-4\"},[_c('figure',{attrs:{\"data-aos\":\"zoom-in\"}},[_c('img',{attrs:{\"src\":require(\"@/assets/curso/tema2/1.png\"),\"alt\":\"\"}})])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"titulo-segundo\",attrs:{\"id\":\"t_2_1\",\"data-aos\":\"flip-up\"}},[_c('h2',[_c('span',[_vm._v(\"2.1\")]),_vm._v(\" Principios claves\")])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row mb-5\"},[_c('div',{staticClass:\"col-lg-4\"},[_c('figure',[_c('img',{attrs:{\"src\":require(\"@/assets/curso/tema2/2.png\"),\"data-aos\":\"zoom-in\"}})])]),_c('div',{staticClass:\"col-lg-8\"},[_c('p',{staticClass:\"mb-4\",attrs:{\"data-aos\":\"fade-left\"}},[_vm._v(\"La responsabilidad es otro principio ético clave que es esencial para el desarrollo responsable de la IA, que establece la rendición de cuentas se refiere a la necesidad de que haya individuos u organizaciones responsables de las acciones de los sistemas de IA.\")]),_c('div',{staticClass:\"bg-color-2 p-4\",attrs:{\"data-aos\":\"fade-left\"}},[_c('p',{staticClass:\"mb-0\"},[_vm._v(\"Esto requiere establecer líneas claras de responsabilidad y desarrollar mecanismos para garantizar que los sistemas de IA se utilizan de forma responsable y ética. Además, los principios éticos para la investigación en IA sanitaria hacen hincapié en la transparencia y la explicabilidad, en los estrictos protocolos que se adhieren a las leyes y reglamentos pertinentes, y en el compromiso de las partes interesadas, es importante extender estos elementos a los diferentes campos de aplicación de la IA.\")])])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row mb-4\"},[_c('div',{staticClass:\"col-lg-8\"},[_c('div',{staticClass:\"bg-color-1 p-4 mb-4\",attrs:{\"data-aos\":\"fade-left\"}},[_c('div',{staticClass:\"row align-items-start mb-4\"},[_c('div',{staticClass:\"col-lg-auto\"},[_c('img',{staticClass:\"mx-auto\",staticStyle:{\"max-width\":\"90px\"},attrs:{\"src\":require(\"@/assets/curso/tema2/4.svg\")}})]),_c('div',{staticClass:\"col-lg\"},[_c('p',{staticClass:\"mb-0\"},[_vm._v(\"Además, se debe garantizar la privacidad y la confidencialidad salvaguardando los datos contra el uso indebido mediante el establecimiento de directrices claras para la gestión de los datos, lo que mitiga los riesgos asociados a las filtraciones de datos y el acceso no autorizado.\")])])]),_c('div',{staticClass:\"row align-items-start mb-4\"},[_c('div',{staticClass:\"col-lg-auto\"},[_c('img',{staticClass:\"mx-auto\",staticStyle:{\"max-width\":\"90px\"},attrs:{\"src\":require(\"@/assets/curso/tema2/5.svg\")}})]),_c('div',{staticClass:\"col-lg\"},[_c('p',{staticClass:\"mb-0\"},[_vm._v(\"Esto en sectores como el de la salud y financiero son de gran importancia, y el establecimiento de normas claras en estos campos, puede permitir generalizar sus resultados para aplicarlos en los demás sectores.\")])])])])]),_c('div',{staticClass:\"col-lg-4\"},[_c('figure',[_c('img',{attrs:{\"src\":require(\"@/assets/curso/tema2/6.png\"),\"data-aos\":\"zoom-in\"}})])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row mb-5\"},[_c('div',{staticClass:\"col-lg-4\"},[_c('figure',[_c('img',{attrs:{\"src\":require(\"@/assets/curso/tema2/7.png\"),\"data-aos\":\"zoom-in\"}})])]),_c('div',{staticClass:\"col-lg-8\"},[_c('p',{staticClass:\"mb-4\",attrs:{\"data-aos\":\"fade-left\"}},[_vm._v(\"La colaboración entre investigadores, profesionales, responsables políticos y expertos en tecnología es esencial para superar los retos éticos y promover el uso responsable de la IA. Como los sistemas de IA son cada vez más sofisticados y están más integrados en la vida humana, es esencial tener en cuenta estas normas éticas para garantizar que la IA se utiliza de forma responsable y para el bien común. Por lo tanto, los desarrolladores de IA deben dar prioridad a estos principios durante todo el ciclo de vida de la IA, desde el diseño hasta el despliegue, para fomentar la confianza y maximizar los beneficios sociales de la IA.\")]),_c('div',{staticClass:\"bg-color-2 p-4\",attrs:{\"data-aos\":\"fade-left\"}},[_c('p',{staticClass:\"mb-0\"},[_vm._v(\"Estos principios son importantes porque permiten a las personas comprender y cuestionar las decisiones de la IA, y responsabilizar a los desarrolladores y usuarios de la IA por sus acciones.\")])])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"titulo-segundo\",attrs:{\"id\":\"t_2_2\",\"data-aos\":\"flip-up\"}},[_c('h2',[_c('span',[_vm._v(\"2.2\")]),_vm._v(\" Naturaleza de los sesgos\")])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('p',{staticClass:\"mb-4\",attrs:{\"data-aos\":\"fade-left\"}},[_vm._v(\"Para conocer sobre esta temática, lo invitamos a escuchar el siguiente \"),_c('em',[_vm._v(\"podcast\")]),_vm._v(\".\")])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"col-lg-auto\"},[_c('figure',[_c('img',{staticStyle:{\"max-width\":\"610px\"},attrs:{\"src\":require(\"@/assets/curso/tema2/8.svg\"),\"data-aos\":\"zoom-in\"}})])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"titulo-segundo\",attrs:{\"id\":\"t_2_3\",\"data-aos\":\"flip-up\"}},[_c('h2',[_c('span',[_vm._v(\"2.3\")]),_vm._v(\" Tipos de sesgo\")])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"col-lg-4\"},[_c('figure',[_c('img',{staticClass:\"mb-4 mb-lg-0\",attrs:{\"src\":require(\"@/assets/curso/tema2/9.png\"),\"alt\":\"\"}})])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"col-lg-4\"},[_c('figure',[_c('img',{staticClass:\"mb-4 mb-lg-0\",attrs:{\"src\":require(\"@/assets/curso/tema2/10.png\"),\"alt\":\"\"}})])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"col-lg-4\"},[_c('figure',[_c('img',{staticClass:\"mb-4 mb-lg-0\",attrs:{\"src\":require(\"@/assets/curso/tema2/11.png\"),\"alt\":\"\"}})])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"bg-full-width bg-color-3 mb-5\"},[_c('div',{staticClass:\"px-4 px-md-5 py-4\"},[_c('div',{staticClass:\"row align-items-center\"},[_c('div',{staticClass:\"col-lg-auto\"},[_c('img',{staticClass:\"mx-auto\",staticStyle:{\"max-width\":\"90px\"},attrs:{\"src\":require(\"@/assets/curso/tema2/13.svg\")}})]),_c('div',{staticClass:\"col-lg\"},[_c('p',{staticClass:\"mb-0\"},[_vm._v(\"En la siguiente tabla se relacionan los diferentes tipos de sesgo, una breve descripción, cuál es su origen principal y las consecuencias que pueden traer.\")])])])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"titulo-figura mb-2\"},[_c('h5',[_vm._v(\"Tabla 1. \")]),_c('span',[_vm._v(\"Comparativa de sesgos en algoritmos de IA\")])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"tabla-a color-acento-contenido mb-5\"},[_c('table',{staticClass:\"bordes\",attrs:{\"id\":\"Tabla1\"}},[_c('thead',[_c('tr',[_c('th',{staticClass:\"bg-color-tabla text-white\"},[_vm._v(\"Nombre del sesgo\")]),_c('th',{staticClass:\"bg-color-tabla text-white\"},[_vm._v(\"Descripción\")]),_c('th',{staticClass:\"bg-color-tabla text-white\"},[_vm._v(\"Origen principal\")]),_c('th',{staticClass:\"bg-color-tabla text-white\"},[_vm._v(\"Consecuencias\")])])]),_c('tbody',[_c('tr',[_c('td',[_c('strong',[_vm._v(\"Responsabilidad\")])]),_c('td',[_vm._v(\"Falta de claridad sobre quién responde por los errores de la IA.\")]),_c('td',[_vm._v(\"Diseño organizacional.\")]),_c('td',[_vm._v(\"Dificultad para rendir cuentas y corregir errores.\")])]),_c('tr',[_c('td',[_c('strong',[_vm._v(\"Exterioridad\")])]),_c('td',[_vm._v(\"No se consideran efectos externos del sistema de IA.\")]),_c('td',[_vm._v(\"Falta de evaluación sistémica.\")]),_c('td',[_vm._v(\"Impactos sociales o ambientales no mitigados.\")])]),_c('tr',[_c('td',[_c('strong',[_vm._v(\"Explicación\")])]),_c('td',[_vm._v(\"Decisiones poco transparentes o difíciles de entender.\")]),_c('td',[_vm._v(\"Modelos opacos (caja negra).\")]),_c('td',[_vm._v(\"Desconfianza, falta de supervisión.\")])]),_c('tr',[_c('td',[_c('strong',[_vm._v(\"Datos\")])]),_c('td',[_vm._v(\"Datos sesgados o incompletos.\")]),_c('td',[_vm._v(\"Recolección y selección de datos.\")]),_c('td',[_vm._v(\"Resultados injustos o inexactos.\")])]),_c('tr',[_c('td',[_c('strong',[_vm._v(\"Confirmación\")])]),_c('td',[_vm._v(\"Refuerzo de supuestos previos al analizar datos.\")]),_c('td',[_vm._v(\"Interpretación humana.\")]),_c('td',[_vm._v(\"Predicciones parciales o injustas.\")])]),_c('tr',[_c('td',[_c('strong',[_vm._v(\"Automatización\")])]),_c('td',[_vm._v(\"Confianza excesiva en la IA.\")]),_c('td',[_vm._v(\"Uso sin supervisión humana.\")]),_c('td',[_vm._v(\"Errores no cuestionados.\")])]),_c('tr',[_c('td',[_c('strong',[_vm._v(\"Representación\")])]),_c('td',[_vm._v(\"Grupos poco o mal representados en los datos.\")]),_c('td',[_vm._v(\"Dataset desequilibrado.\")]),_c('td',[_vm._v(\"Discriminación indirecta.\")])]),_c('tr',[_c('td',[_c('strong',[_vm._v(\"Algoritmo\")])]),_c('td',[_vm._v(\"Sesgos por reglas o parámetros del algoritmo.\")]),_c('td',[_vm._v(\"Diseño del modelo.\")]),_c('td',[_vm._v(\"Resultados parcializados.\")])]),_c('tr',[_c('td',[_c('strong',[_vm._v(\"Observación\")])]),_c('td',[_vm._v(\"Forma en que se recolectan los datos influye en los resultados.\")]),_c('td',[_vm._v(\"Métodos de medición.\")]),_c('td',[_vm._v(\"Poca generalización o precisión.\")])])])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"titulo-segundo\",attrs:{\"id\":\"t_2_4\",\"data-aos\":\"flip-up\"}},[_c('h2',[_c('span',[_vm._v(\"2.4\")]),_vm._v(\" Estrategias para mitigar los sesgos\")])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row mb-5\"},[_c('div',{staticClass:\"col-lg-7\"},[_c('div',{staticClass:\"bg-color-2 px-4 py-5 mb-4\",attrs:{\"data-aos\":\"fade-left\"}},[_c('p',[_vm._v(\"Mitigar los sesgos en algoritmos de Inteligencia Artificial, es clave para desarrollar sistemas más justos, seguros y responsables. Afortunadamente, hay muchas estrategias técnicas, éticas, legales y organizacionales, que se pueden aplicar en diferentes etapas del desarrollo de la IA.\")]),_c('p',{staticClass:\"mb-0\"},[_vm._v(\"A continuación, se describen las principales estrategias para mitigar sesgos en IA, agrupadas por fases del ciclo de vida del sistema.\")])])]),_c('div',{staticClass:\"col-lg-5 d-none d-sm-block\"},[_c('figure',[_c('img',{staticClass:\"mb-4 mb-lg-0\",attrs:{\"src\":require(\"@/assets/curso/tema2/14.png\"),\"data-aos\":\"zoom-in\"}})])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"bg-full-width bg-fondo-3 mb-5\"},[_c('div',{staticClass:\"p-4 p-md-5\"},[_c('h2',[_vm._v(\"Durante la recolección de datos\")]),_c('p',[_vm._v(\"Los datos son la base del modelo. Si están sesgados, el sistema también lo estará, por esto se plantean las siguientes estrategias:\")]),_c('div',{staticClass:\"row justify-content-center align-items-stretch mb-5\"},[_c('div',{staticClass:\"col-lg-3 mb-4\",attrs:{\"data-aos\":\"zoom-in-up\"}},[_c('div',{staticClass:\"bg-color-white box-shadow px-4 py-5 h-100\"},[_c('img',{staticClass:\"mx-auto d-block mb-4\",staticStyle:{\"width\":\"90px\"},attrs:{\"src\":require(\"@/assets/curso/tema2/15.svg\"),\"alt\":\"\"}}),_c('h5',{staticClass:\"text-center\"},[_vm._v(\"Auditoría de datos\")]),_c('p',{staticClass:\"mb-0 text-center\"},[_vm._v(\"Analizar los datos para detectar desbalance, omisiones o representaciones injustas.\")])])]),_c('div',{staticClass:\"col-lg-3 mb-4\",attrs:{\"data-aos\":\"zoom-in-up\"}},[_c('div',{staticClass:\"bg-color-white box-shadow px-4 py-5 h-100\"},[_c('img',{staticClass:\"mx-auto d-block mb-4\",staticStyle:{\"width\":\"90px\"},attrs:{\"src\":require(\"@/assets/curso/tema2/16.svg\"),\"alt\":\"\"}}),_c('h5',{staticClass:\"text-center\"},[_vm._v(\"Diversificación de fuentes\")]),_c('p',{staticClass:\"mb-0 text-center\"},[_vm._v(\"Incluir datos de distintas poblaciones, contextos geográficos, culturas, géneros, etc.\")])])]),_c('div',{staticClass:\"col-lg-3 mb-4\",attrs:{\"data-aos\":\"zoom-in-up\"}},[_c('div',{staticClass:\"bg-color-white box-shadow px-4 py-5 h-100\"},[_c('img',{staticClass:\"mx-auto d-block mb-4\",staticStyle:{\"width\":\"90px\"},attrs:{\"src\":require(\"@/assets/curso/tema2/17.svg\"),\"alt\":\"\"}}),_c('h5',{staticClass:\"text-center\"},[_vm._v(\"Anotación cuidadosa\")]),_c('p',{staticClass:\"mb-0 text-center\"},[_vm._v(\"Entrenar a los etiquetadores para evitar prejuicios inconscientes.\")])])]),_c('div',{staticClass:\"col-lg-3 mb-4\",attrs:{\"data-aos\":\"zoom-in-up\"}},[_c('div',{staticClass:\"bg-color-white box-shadow px-4 py-5 h-100\"},[_c('img',{staticClass:\"mx-auto d-block mb-4\",staticStyle:{\"width\":\"90px\"},attrs:{\"src\":require(\"@/assets/curso/tema2/18.svg\"),\"alt\":\"\"}}),_c('h5',{staticClass:\"text-center\"},[_vm._v(\"Datos sintéticos\")]),_c('p',{staticClass:\"mb-0 text-center\"},[_vm._v(\"Generar ejemplos balanceados usando técnicas como GANs cuando hay escasez de ciertos grupos.\")])])])])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row justify-content-center align-items-stretch mb-4\"},[_c('div',{staticClass:\"col-lg-6\"},[_c('div',{staticClass:\"bg-color-1 p-4 h-100\",attrs:{\"data-aos\":\"fade-left\"}},[_c('div',{staticClass:\"row align-items-center\"},[_c('div',{staticClass:\"col-lg-auto\"},[_c('img',{staticClass:\"mx-auto\",staticStyle:{\"max-width\":\"90px\"},attrs:{\"src\":require(\"@/assets/curso/tema2/19.svg\")}})]),_c('div',{staticClass:\"col-lg\"},[_c('h5',{staticClass:\"mb-2\"},[_vm._v(\"Monitoreo constante\")]),_c('p',{staticClass:\"mb-0\"},[_vm._v(\"Revisar en tiempo real si el modelo sigue funcionando de forma justa y precisa.\")])])])])]),_c('div',{staticClass:\"col-lg-6\"},[_c('div',{staticClass:\"bg-color-1 p-4 h-100\",attrs:{\"data-aos\":\"fade-left\"}},[_c('div',{staticClass:\"row align-items-center\"},[_c('div',{staticClass:\"col-lg-auto\"},[_c('img',{staticClass:\"mx-auto\",staticStyle:{\"max-width\":\"90px\"},attrs:{\"src\":require(\"@/assets/curso/tema2/20.svg\")}})]),_c('div',{staticClass:\"col-lg\"},[_c('h5',{staticClass:\"mb-2\"},[_c('em',[_vm._v(\"Feedback\")]),_vm._v(\" humano\")]),_c('p',{staticClass:\"mb-0\"},[_vm._v(\"Incluir supervisión humana para revisar decisiones críticas (como rechazos de crédito o diagnósticos médicos).\")])])])])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"row justify-content-center align-items-stretch mb-5\"},[_c('div',{staticClass:\"col-lg-6\"},[_c('div',{staticClass:\"bg-color-1 p-4 h-100\",attrs:{\"data-aos\":\"fade-left\"}},[_c('div',{staticClass:\"row align-items-center\"},[_c('div',{staticClass:\"col-lg-auto\"},[_c('img',{staticClass:\"mx-auto\",staticStyle:{\"max-width\":\"90px\"},attrs:{\"src\":require(\"@/assets/curso/tema2/21.svg\")}})]),_c('div',{staticClass:\"col-lg\"},[_c('h5',{staticClass:\"mb-2\"},[_vm._v(\"Canales de apelación\")]),_c('p',{staticClass:\"mb-0\"},[_vm._v(\"Permitir a los usuarios cuestionar o apelar decisiones algorítmicas.\")])])])])]),_c('div',{staticClass:\"col-lg-6\"},[_c('div',{staticClass:\"bg-color-1 p-4 h-100\",attrs:{\"data-aos\":\"fade-left\"}},[_c('div',{staticClass:\"row align-items-center\"},[_c('div',{staticClass:\"col-lg-auto\"},[_c('img',{staticClass:\"mx-auto\",staticStyle:{\"max-width\":\"90px\"},attrs:{\"src\":require(\"@/assets/curso/tema2/22.svg\")}})]),_c('div',{staticClass:\"col-lg\"},[_c('h5',{staticClass:\"mb-2\"},[_vm._v(\"Pruebas en contextos reales\")]),_c('p',{staticClass:\"mb-0\"},[_vm._v(\"Hacer pruebas piloto antes del despliegue total, para ver cómo el sistema impacta a diferentes comunidades.\")])])])])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"bg-full-width-2 bg-fondo-4\"},[_c('div',{staticClass:\"px-4 px-md-5 py-4\"},[_c('h2',[_vm._v(\"A nivel organizacional y ético\")]),_c('p',[_vm._v(\"Incluso los mejores modelos pueden fallar si no hay una cultura responsable, es por esto por lo que se deben implementar las siguientes estrategias:\")]),_c('div',{staticClass:\"row justify-content-center align-items-stretch mb-5\"},[_c('div',{staticClass:\"col-lg-3 mb-4\",attrs:{\"data-aos\":\"zoom-in-up\"}},[_c('div',{staticClass:\"custom-image-card-2 h-100\"},[_c('img',{staticClass:\"custom-image-card__image\",attrs:{\"src\":require(\"@/assets/curso/tema2/23.png\"),\"alt\":\"\"}}),_c('div',{staticClass:\"custom-image-card__text p-4\"},[_c('h5',{staticClass:\"mb-2 text-center\"},[_vm._v(\"Equipos diversos\")]),_c('p',{staticClass:\"mb-0 text-center\"},[_vm._v(\"Incluir personas de distintos géneros, culturas, profesiones y trayectorias, para reducir sesgos desde la concepción del sistema.\")])])])]),_c('div',{staticClass:\"col-lg-3 mb-4\",attrs:{\"data-aos\":\"zoom-in-down\"}},[_c('div',{staticClass:\"custom-image-card-2 h-100\"},[_c('img',{staticClass:\"custom-image-card__image\",attrs:{\"src\":require(\"@/assets/curso/tema2/24.png\"),\"alt\":\"\"}}),_c('div',{staticClass:\"custom-image-card__text p-4\"},[_c('h5',{staticClass:\"mb-2 text-center\"},[_vm._v(\"Ética por diseño\")]),_c('p',{staticClass:\"mb-0 text-center\"},[_vm._v(\"Integrar principios éticos (justicia, privacidad, responsabilidad) desde el principio del desarrollo.\")])])])]),_c('div',{staticClass:\"col-lg-3 mb-4\",attrs:{\"data-aos\":\"zoom-in-down\"}},[_c('div',{staticClass:\"custom-image-card-2 h-100\"},[_c('img',{staticClass:\"custom-image-card__image\",attrs:{\"src\":require(\"@/assets/curso/tema2/25.png\"),\"alt\":\"\"}}),_c('div',{staticClass:\"custom-image-card__text p-4\"},[_c('h5',{staticClass:\"mb-2 text-center\"},[_vm._v(\"Transparencia y documentación\")]),_c('p',{staticClass:\"mb-0 text-center\"},[_vm._v(\"Usar herramientas como \"),_c('em',[_vm._v(\"Model Cards y Datasheets for Datasets\")]),_vm._v(\", para documentar cómo se construyó el sistema y qué limitaciones tiene.\")])])])]),_c('div',{staticClass:\"col-lg-3 mb-4\",attrs:{\"data-aos\":\"zoom-in-down\"}},[_c('div',{staticClass:\"custom-image-card-2 h-100\"},[_c('img',{staticClass:\"custom-image-card__image\",attrs:{\"src\":require(\"@/assets/curso/tema2/26.png\"),\"alt\":\"\"}}),_c('div',{staticClass:\"custom-image-card__text p-4\"},[_c('h5',{staticClass:\"mb-2 text-center\"},[_vm._v(\"Cumplimiento legal\")]),_c('p',{staticClass:\"mb-0 text-center\"},[_vm._v(\"Asegurar que el sistema cumple con normas como el Reglamento General de Protección de Datos (GDPR), leyes antidiscriminatorias, etc.\")])])])])])])])},function () {var _vm=this;var _h=_vm.$createElement;var _c=_vm._self._c||_h;return _c('div',{staticClass:\"col-12 col-md-6 col-lg-3 offset-lg-1\"},[_c('figure',[_c('img',{attrs:{\"src\":require(\"@/assets/componentes/material-complementario.svg\"),\"alt\":\"\"}})])])}]\n\nexport { render, staticRenderFns }","<template lang=\"pug\">\n  .curso-main-container.pb-3\n    BannerInterno(:subTitulo=\"'2. Sesgos en los algoritmos: cómo se generan y sus implicaciones'\")\n    .container.tarjeta.tarjeta--blanca.p-4.p-md-5.overflow-hidden\n\n      p La presencia de sesgos en los algoritmos de inteligencia artificial representa un desafío crítico para garantizar la equidad, la transparencia y la ética en su aplicación.\n\n\n      .bg-full-width.bg-color-6.mb-lg-5\n        .px-4.p-md-5\n          .row.justify-content-center.align-items-center\n            .col-lg-8\n              h2.mb-4(data-aos=\"flip-up\").text-white Sesgos en los algoritmos: cómo se generan y sus implicaciones\n              p.mb-4(data-aos=\"fade-right\") En el PDF Sesgos en los algoritmos, se examinan las fuentes de estos sesgos, sus efectos en ámbitos clave como la contratación, la justicia y la salud, así como las estrategias para mitigarlos mediante enfoques técnicos, éticos y normativos. Este análisis proporciona herramientas fundamentales para promover un uso más justo y responsable de la IA en la sociedad.\n      \n              a.anexo.mb-4.bg-white.w-fit(:href=\"obtenerLink('/downloads/Anexo_2.pdf')\" target=\"_blank\")(data-aos=\"flip-up\")\n                .anexo__icono(:style=\"{'background-color': '#FCDFDB'}\")\n                  img(src=\"@/assets/template/icono-pdf.svg\")\n                .anexo__texto\n                  p <strong>Anexo. </strong> Sesgos en los algoritmos\n      \n            .col-lg-4\n              figure(data-aos=\"zoom-in\")\n                img(src='@/assets/curso/tema2/1.png', alt='')\n      \n  \n      #t_2_1.titulo-segundo(data-aos=\"flip-up\")\n        h2 #[span 2.1] Principios claves\n\n\n      p La transparencia y la explicabilidad son principios éticos claves que son esenciales para el desarrollo responsable de la IA. La disponibilidad a gran escala de datos de comportamiento humano y las mayores capacidades de la inteligencia artificial están permitiendo a investigadores, empresas, profesionales y responsables políticos co-desarrollar y evaluar en el mundo real procesos algorítmicos de toma de decisiones diseñados para maximizar la equidad, la rendición de cuentas y la transparencia, respetando al mismo tiempo la privacidad. A continuación, conozcamos los principales:\n\n\n      .bg-full-width.bg-color-info.mb-5\n        .p-4.p-md-5\n          .row.justify-content-center.align-items-center.mb-5\n            .col-lg-10\n              ImagenInfografica.color-secundario\n                template(v-slot:imagen)\n                  figure\n                    img(src='@/assets/curso/tema2/info1.png', alt='', style=\"max-width: 873px;\").mx-auto\n      \n                .bg-color-white.box-shadow.p-3(x=\"10%\" y=\"64%\" numero=\"+\")\n                  h5 01. Transparencia\n                  p Posibilidad de comprender cómo funcionan los algoritmos de IA y cómo toman decisiones.\n      \n      \n                .bg-color-white.box-shadow.p-3(x=\"75%\" y=\"23%\" numero=\"+\")\n                  h5 02. Explicabilidad\n                  p Posibilidad de explicar las decisiones de la IA a las personas afectadas por ellas.\n      \n\n      \n                .bg-color-white.box-shadow.p-3(x=\"90%\" y=\"64%\" numero=\"+\")\n                  h5 03. Rendición de cuentas\n                  p Necesidad de que haya individuos u organizaciones responsables de las acciones de los sistemas de IA.\n      \n\n      .row.mb-5\n        .col-lg-4\n          figure\n            img(src=\"@/assets/curso/tema2/2.png\", data-aos=\"zoom-in\")\n        .col-lg-8\n          p(data-aos=\"fade-left\").mb-4 La responsabilidad es otro principio ético clave que es esencial para el desarrollo responsable de la IA, que establece la rendición de cuentas se refiere a la necesidad de que haya individuos u organizaciones responsables de las acciones de los sistemas de IA.\n      \n          .bg-color-2.p-4(data-aos=\"fade-left\")\n            p.mb-0 Esto requiere establecer líneas claras de responsabilidad y desarrollar mecanismos para garantizar que los sistemas de IA se utilizan de forma responsable y ética. Además, los principios éticos para la investigación en IA sanitaria hacen hincapié en la transparencia y la explicabilidad, en los estrictos protocolos que se adhieren a las leyes y reglamentos pertinentes, y en el compromiso de las partes interesadas, es importante extender estos elementos a los diferentes campos de aplicación de la IA.\n      \n      p La rendición de cuentas puede demostrarse mediante una gestión eficaz, una supervisión periódica, la mitigación de riesgos, la evaluación del impacto, la comunicación abierta y la reparación de daños.\n\n\n      .row.mb-4\n        .col-lg-8\n          .bg-color-1.p-4.mb-4(data-aos=\"fade-left\")\n            .row.align-items-start.mb-4\n              .col-lg-auto\n                img(src=\"@/assets/curso/tema2/4.svg\", style=\"max-width: 90px\").mx-auto\n              .col-lg\n                p.mb-0 Además, se debe garantizar la privacidad y la confidencialidad salvaguardando los datos contra el uso indebido mediante el establecimiento de directrices claras para la gestión de los datos, lo que mitiga los riesgos asociados a las filtraciones de datos y el acceso no autorizado.\n      \n            .row.align-items-start.mb-4\n              .col-lg-auto\n                img(src=\"@/assets/curso/tema2/5.svg\", style=\"max-width: 90px\").mx-auto\n              .col-lg\n                p.mb-0 Esto en sectores como el de la salud y financiero son de gran importancia, y el establecimiento de normas claras en estos campos, puede permitir generalizar sus resultados para aplicarlos en los demás sectores.\n\n\n      \n        .col-lg-4\n          figure\n            img(src=\"@/assets/curso/tema2/6.png\", data-aos=\"zoom-in\")\n      \n\n\n      .row.mb-5\n        .col-lg-4\n          figure\n            img(src=\"@/assets/curso/tema2/7.png\", data-aos=\"zoom-in\")\n        .col-lg-8\n          p(data-aos=\"fade-left\").mb-4 La colaboración entre investigadores, profesionales, responsables políticos y expertos en tecnología es esencial para superar los retos éticos y promover el uso responsable de la IA. Como los sistemas de IA son cada vez más sofisticados y están más integrados en la vida humana, es esencial tener en cuenta estas normas éticas para garantizar que la IA se utiliza de forma responsable y para el bien común. Por lo tanto, los desarrolladores de IA deben dar prioridad a estos principios durante todo el ciclo de vida de la IA, desde el diseño hasta el despliegue, para fomentar la confianza y maximizar los beneficios sociales de la IA.\n      \n          .bg-color-2.p-4(data-aos=\"fade-left\")\n            p.mb-0 Estos principios son importantes porque permiten a las personas comprender y cuestionar las decisiones de la IA, y responsabilizar a los desarrolladores y usuarios de la IA por sus acciones.\n      \n\n\n\n      .bg-full-width.bg-fondo-2.mb-5\n        .p-4.p-md-5\n\n          #t_2_2.titulo-segundo(data-aos=\"flip-up\")\n            h2 #[span 2.2] Naturaleza de los sesgos\n\n          .row.justify-content-center.align-items-center\n\n            .col-lg\n              p(data-aos=\"fade-left\").mb-4 Para conocer sobre esta temática, lo invitamos a escuchar el siguiente #[em podcast].\n\n              \n              TarjetaAudio.color-acento-botones.bg-color-white.mb-3(\n                texto=\"Naturaleza de los sesgos\"\n                tiempo\n                :audio=\"require('../../assets/curso/podcast/podcast1.mp3')\"\n              )\n            .col-lg-auto\n              figure\n                img(src=\"@/assets/curso/tema2/8.svg\", data-aos=\"zoom-in\", style=\"max-width: 610px;\")\n\n\n      #t_2_3.titulo-segundo(data-aos=\"flip-up\")\n        h2 #[span 2.3] Tipos de sesgo\n\n\n      p Existen diferentes tipos de sesgo en los algoritmos de IA, a continuación, se tratarán de forma global los principales.\n\n\n      .row.align-items-start.mb-5\n        .col-lg-4\n          figure\n            img(src=\"@/assets/curso/tema2/9.png\", alt=\"\").mb-4.mb-lg-0\n        .col-lg-8\n\n      \n          AcordionA(tipo=\"b\")#Acordeon1\n            .div(titulo=\"Sesgo de representatividad\")\n              p Este fenómeno se produce cuando el sistema predictivo reduce la cantidad de datos utilizados por su naturaleza representativa, lo que puede influir negativamente en la validez estadístico-inductiva del modelo desarrollado.\n              p\n                strong Ejemplo. \n                | Podría ser el caso en el que se llevan a cabo devoluciones de productos en plataformas de comercio electrónico. Si se decide utilizar únicamente esta información para clasificar a un consumidor específico, se puede caer en la trampa de considerar a ese individuo como un cliente familiarizado. Esto ocurre debido a que el co-sistema en uso presenta un sesgo representativo que impacta negativamente en la línea de productos del sistema analizado, llevando a conclusiones que no reflejan la realidad general de todos los consumidores.\n            .div(titulo=\"Sesgo de exterioridad (ambiental)\")\n              p Esto ocurre cuando se excluyen ciertas características relevantes, lo que puede comprometer la validez de diversas relaciones que están presentes entre el ambiente que produce dicho sesgo y el efecto específico que se busca predecir.\n              p\n                strong Ejemplo. \n                | En un sistema predictivo de créditos, si no se incorporan adecuadamente los resultados de investigaciones económicas pertinentes, esto podría llevar a una mala asignación de las reputaciones crediticias de los usuarios. Esto no solo afecta a los individuos, sino que puede ocasionar repercusiones más amplias en el sistema financiero. Además, el sesgo puede derivar en decisiones que no reflejan la realidad económica y social que rodea a los solicitantes de crédito.\n            .div(titulo=\"Sesgo de explicación (determinismo funcional)\")\n              p Este fenómeno ocurre cuando dentro de los sistemas se excluyen grupos de individuos cuyos estímulos pueden ser similares o cuando, de alguna manera, se minimizan las predicciones realizadas por el sistema respecto a los resultados, lo que compromete de manera significativa la validez de las elecciones que realiza el usuario. Esto intenta incentivar para que esas elecciones se mantengan firmes y no se vean afectadas por las predicciones del sistema.\n              p\n                strong Ejemplo. \n                | Un ejemplo claro de esto es un sistema de entretenimiento cuyo software ha sido diseñado de tal manera que limita la exposición a ciertos contenidos, buscando influir en las preferencias del usuario y en sus decisiones subsiguientes.\n      \n\n\n      .row.align-items-start.mb-5\n        .col-lg-8\n\n      \n          AcordionA(tipo=\"b\")#Acordeon2\n            .div(titulo=\"Sesgo de responsabilidad\")\n              p Este sesgo surge cuando no está claro quién debe asumir la responsabilidad por los resultados de un sistema de IA, especialmente cuando esos resultados son perjudiciales. La Inteligencia Artificial, por su naturaleza, implica muchas capas de participación: diseñadores, ingenieros, empresas, usuarios, reguladores, etc. Esta multiplicidad de actores puede provocar una especie de \"zona gris\" legal y ética.\n              p\n                strong Ejemplo. \n                | Si un coche autónomo atropella a una persona, ¿quién tiene la culpa? ¿El fabricante del coche? ¿El programador del algoritmo de frenado? ¿El dueño del vehículo?\n            .div(titulo=\"Sesgo de datos\")\n              p Este es uno de los más comunes. Se produce cuando los datos usados para entrenar una IA son incompletos, desequilibrados o reflejan prejuicios existentes en la sociedad. El sistema aprende esos sesgos como si fueran verdades objetivas.\n              p\n                strong Ejemplo. \n                | Si un sistema de contratación se entrena con datos de empleados históricos, y estos reflejan una preferencia por hombres, el modelo aprenderá a preferir hombres.\n            .div(titulo=\"Sesgo de confirmación\")\n              p Este sesgo ocurre cuando el modelo (o las personas que lo usan) interpreta los resultados de la IA de forma que refuerza sus creencias previas, en lugar de cuestionarlas. A menudo, los sistemas son entrenados o ajustados para confirmar hipótesis iniciales en lugar de desafiarlas.\n              p\n                strong Ejemplo. \n                | Un sistema predictivo policial que detecta más crimen en barrios pobres simplemente porque históricamente se ha hecho más vigilancia ahí. El modelo refuerza esa suposición y sigue enviando más patrullas, aunque el crimen real no haya aumentado.\n\n      \n        .col-lg-4\n          figure\n            img(src=\"@/assets/curso/tema2/10.png\", alt=\"\").mb-4.mb-lg-0\n      \n\n      .row.align-items-start.mb-5\n        .col-lg-4\n          figure\n            img(src=\"@/assets/curso/tema2/11.png\", alt=\"\").mb-4.mb-lg-0\n        .col-lg-8\n      \n          AcordionA(tipo=\"b\")#Acordeon3\n            .div(titulo=\"Sesgo de automatización\")\n              p Este sesgo se refiere a la tendencia de los humanos a confiar demasiado en decisiones automatizadas, incluso cuando tienen dudas o cuando esas decisiones contradicen la lógica o los hechos.\n              p\n                strong Ejemplo. \n                | Un médico sigue la recomendación de una IA diagnóstica, aunque su experiencia le indique lo contrario, simplemente porque \"la IA debe saber más\".\n            .div(titulo=\"Sesgo del algoritmo\")\n              p Este sesgo aparece cuando las propias reglas, parámetros o arquitectura del algoritmo favorecen ciertos resultados. Incluso con buenos datos, un algoritmo mal diseñado puede producir decisiones injustas.\n              p\n                strong Ejemplo. \n                | Un sistema de búsqueda que prioriza resultados por número de clics, lo cual favorece información sensacionalista o sesgada.\n            .div(titulo=\"Sesgo de observación\")\n              p Este sesgo se presenta cuando la forma en que se recolectan los datos introduce distorsiones, es decir, cuando la observación misma afecta el fenómeno o los datos obtenidos.\n              p\n                strong Ejemplo. \n                | Un sistema de salud que se entrena solo con datos de pacientes de hospitales privados probablemente no refleje los problemas de salud de poblaciones rurales o sin acceso.\n\n\n      .bg-full-width.bg-color-3.mb-5\n        .px-4.px-md-5.py-4\n          .row.align-items-center\n            .col-lg-auto\n              img(src=\"@/assets/curso/tema2/13.svg\", style=\"max-width: 90px\").mx-auto\n            .col-lg\n              p.mb-0 En la siguiente tabla se relacionan los diferentes tipos de sesgo, una breve descripción, cuál es su origen principal y las consecuencias que pueden traer.\n      \n      .titulo-figura.mb-2\n        h5 Tabla 1. \n        span Comparativa de sesgos en algoritmos de IA\n\n\n      .tabla-a.color-acento-contenido.mb-5\n        table#Tabla1.bordes\n          thead\n            tr\n              th.bg-color-tabla.text-white Nombre del sesgo\n              th.bg-color-tabla.text-white Descripción\n              th.bg-color-tabla.text-white Origen principal\n              th.bg-color-tabla.text-white Consecuencias\n          tbody\n            tr\n              td #[strong Responsabilidad]\n              td Falta de claridad sobre quién responde por los errores de la IA.\n              td Diseño organizacional.\n              td Dificultad para rendir cuentas y corregir errores.\n            tr\n              td #[strong Exterioridad]\n              td No se consideran efectos externos del sistema de IA.\n              td Falta de evaluación sistémica.\n              td Impactos sociales o ambientales no mitigados.\n            tr\n              td #[strong Explicación]\n              td Decisiones poco transparentes o difíciles de entender.\n              td Modelos opacos (caja negra).\n              td Desconfianza, falta de supervisión.\n            tr\n              td #[strong Datos]\n              td Datos sesgados o incompletos.\n              td Recolección y selección de datos.\n              td Resultados injustos o inexactos.\n            tr\n              td #[strong Confirmación]\n              td Refuerzo de supuestos previos al analizar datos.\n              td Interpretación humana.\n              td Predicciones parciales o injustas.\n            tr\n              td #[strong Automatización]\n              td Confianza excesiva en la IA.\n              td Uso sin supervisión humana.\n              td Errores no cuestionados.\n            tr\n              td #[strong Representación]\n              td Grupos poco o mal representados en los datos.\n              td Dataset desequilibrado.\n              td Discriminación indirecta.\n            tr\n              td #[strong Algoritmo]\n              td Sesgos por reglas o parámetros del algoritmo.\n              td Diseño del modelo.\n              td Resultados parcializados.\n            tr\n              td #[strong Observación]\n              td Forma en que se recolectan los datos influye en los resultados.\n              td Métodos de medición.\n              td Poca generalización o precisión.\n      \n\n      #t_2_4.titulo-segundo(data-aos=\"flip-up\")\n        h2 #[span 2.4] Estrategias para mitigar los sesgos\n\n\n      .row.mb-5\n        .col-lg-7\n          .bg-color-2.px-4.py-5(data-aos=\"fade-left\").mb-4\n            p Mitigar los sesgos en algoritmos de Inteligencia Artificial, es clave para desarrollar sistemas más justos, seguros y responsables. Afortunadamente, hay muchas estrategias técnicas, éticas, legales y organizacionales, que se pueden aplicar en diferentes etapas del desarrollo de la IA.\n            p.mb-0 A continuación, se describen las principales estrategias para mitigar sesgos en IA, agrupadas por fases del ciclo de vida del sistema.\n          \n        .col-lg-5.d-none.d-sm-block\n          figure\n            img(src=\"@/assets/curso/tema2/14.png\", data-aos=\"zoom-in\").mb-4.mb-lg-0\n      \n      \n      .bg-full-width.bg-fondo-3.mb-5\n        .p-4.p-md-5\n          h2 Durante la recolección de datos\n          p Los datos son la base del modelo. Si están sesgados, el sistema también lo estará, por esto se plantean las siguientes estrategias:\n\n\n          div.row.justify-content-center.align-items-stretch.mb-5\n            div.col-lg-3.mb-4(data-aos=\"zoom-in-up\")\n              div.bg-color-white.box-shadow.px-4.py-5.h-100\n                img.mx-auto.d-block.mb-4(\n                  src=\"@/assets/curso/tema2/15.svg\"\n                  alt=\"\"\n                  style=\"width: 90px\"\n                )\n                h5.text-center Auditoría de datos\n                p.mb-0.text-center Analizar los datos para detectar desbalance, omisiones o representaciones injustas.\n          \n            div.col-lg-3.mb-4(data-aos=\"zoom-in-up\")\n              div.bg-color-white.box-shadow.px-4.py-5.h-100\n                img.mx-auto.d-block.mb-4(\n                  src=\"@/assets/curso/tema2/16.svg\"\n                  alt=\"\"\n                  style=\"width: 90px\"\n                )\n                h5.text-center Diversificación de fuentes\n                p.mb-0.text-center Incluir datos de distintas poblaciones, contextos geográficos, culturas, géneros, etc.\n          \n            div.col-lg-3.mb-4(data-aos=\"zoom-in-up\")\n              div.bg-color-white.box-shadow.px-4.py-5.h-100\n                img.mx-auto.d-block.mb-4(\n                  src=\"@/assets/curso/tema2/17.svg\"\n                  alt=\"\"\n                  style=\"width: 90px\"\n                )\n                h5.text-center Anotación cuidadosa\n                p.mb-0.text-center Entrenar a los etiquetadores para evitar prejuicios inconscientes.\n            \n            div.col-lg-3.mb-4(data-aos=\"zoom-in-up\")\n              div.bg-color-white.box-shadow.px-4.py-5.h-100\n                img.mx-auto.d-block.mb-4(\n                  src=\"@/assets/curso/tema2/18.svg\"\n                  alt=\"\"\n                  style=\"width: 90px\"\n                )\n                h5.text-center Datos sintéticos\n                p.mb-0.text-center Generar ejemplos balanceados usando técnicas como GANs cuando hay escasez de ciertos grupos.\n\n\n\n      h2 Durante el diseño y entrenamiento del modelo\n      p En esta etapa es donde se establecen los parámetros y estructuras que pueden amplificar sesgos, algunas estrategias para mitigar los sesgos en esta fase, son:\n\n      .bg-full-width.bg-fondo-slider.mb-5\n        .p-4.p-md-5\n          SlyderA(tipo=\"b\").bg-white\n            .row.align-items-center.p-4.p-md-5\n              .col-lg-5\n                figure\n                  img(src=\"@/assets/curso/tema2/slide-1.png\")\n              .col-lg-7\n                h4 Equidad algorítmica\n                p Aplicar técnicas de fairness como\n                  em reweighing\n                  | ,\n                  em adversarial debiasing\n                  | ,\n                  em preprocessing\n                  |  y\n                  em postprocessing\n                  |  para corregir sesgos matemáticamente.\n            .row.align-items-center.p-4.p-md-5\n              .col-lg-5\n                figure\n                  img(src=\"@/assets/curso/tema2/slide-2.png\")\n              .col-lg-7\n                h4 Regularización con métricas de equidad\n                p Incluir penalizaciones cuando el modelo trata injustamente a ciertos grupos.\n            .row.align-items-center.p-4.p-md-5\n              .col-lg-5\n                figure\n                  img(src=\"@/assets/curso/tema2/slide-3.png\")\n              .col-lg-7\n                h4 Evaluación con métricas justas\n                p Usar métricas como\n                  em equalized odds\n                  | ,\n                  em demographic parity\n                  |  o\n                  em disparate impact\n                  | , para medir discriminación algorítmica.\n            .row.align-items-center.p-4.p-md-5\n              .col-lg-5\n                figure\n                  img(src=\"@/assets/curso/tema2/slide-4.png\")\n              .col-lg-7\n                h4 Modelos interpretables\n                p Favorecer modelos transparentes como árboles de decisión o regresiones, antes que cajas negras.\n      \n\n\n      h2 Durante el despliegue y uso\n      p Un sistema puede funcionar bien durante el entrenamiento, pero fallar en la vida real si las condiciones cambian; por esto se plantean las siguientes estrategias, para ayudar a reducir estos riesgos:\n\n      .row.justify-content-center.align-items-stretch.mb-4\n        .col-lg-6\n          .bg-color-1.p-4(data-aos=\"fade-left\").h-100\n            .row.align-items-center\n              .col-lg-auto\n                img(src=\"@/assets/curso/tema2/19.svg\", style=\"max-width: 90px\").mx-auto\n              .col-lg\n                h5.mb-2 Monitoreo constante\n                p.mb-0 Revisar en tiempo real si el modelo sigue funcionando de forma justa y precisa.\n      \n        .col-lg-6\n          .bg-color-1.p-4(data-aos=\"fade-left\").h-100\n            .row.align-items-center\n              .col-lg-auto\n                img(src=\"@/assets/curso/tema2/20.svg\", style=\"max-width: 90px\").mx-auto\n              .col-lg\n                h5.mb-2 #[em Feedback] humano\n                p.mb-0 Incluir supervisión humana para revisar decisiones críticas (como rechazos de crédito o diagnósticos médicos).\n\n      .row.justify-content-center.align-items-stretch.mb-5\n        .col-lg-6\n          .bg-color-1.p-4(data-aos=\"fade-left\").h-100\n            .row.align-items-center\n              .col-lg-auto\n                img(src=\"@/assets/curso/tema2/21.svg\", style=\"max-width: 90px\").mx-auto\n              .col-lg\n                h5.mb-2 Canales de apelación\n                p.mb-0 Permitir a los usuarios cuestionar o apelar decisiones algorítmicas.\n      \n        .col-lg-6\n          .bg-color-1.p-4(data-aos=\"fade-left\").h-100\n            .row.align-items-center\n              .col-lg-auto\n                img(src=\"@/assets/curso/tema2/22.svg\", style=\"max-width: 90px\").mx-auto\n              .col-lg\n                h5.mb-2 Pruebas en contextos reales\n                p.mb-0 Hacer pruebas piloto antes del despliegue total, para ver cómo el sistema impacta a diferentes comunidades.\n      \n\n      .bg-full-width-2.bg-fondo-4\n        .px-4.px-md-5.py-4\n          h2 A nivel organizacional y ético\n          p Incluso los mejores modelos pueden fallar si no hay una cultura responsable, es por esto por lo que se deben implementar las siguientes estrategias:\n\n\n          .row.justify-content-center.align-items-stretch.mb-5\n            .col-lg-3.mb-4(data-aos=\"zoom-in-up\")\n              .custom-image-card-2.h-100\n                img.custom-image-card__image(src=\"@/assets/curso/tema2/23.png\" alt=\"\")\n                .custom-image-card__text.p-4\n                  h5.mb-2.text-center Equipos diversos\n                  p.mb-0.text-center Incluir personas de distintos géneros, culturas, profesiones y trayectorias, para reducir sesgos desde la concepción del sistema.\n            .col-lg-3.mb-4(data-aos=\"zoom-in-down\")\n              .custom-image-card-2.h-100\n                img.custom-image-card__image(src=\"@/assets/curso/tema2/24.png\" alt=\"\")\n                .custom-image-card__text.p-4\n                  h5.mb-2.text-center Ética por diseño\n                  p.mb-0.text-center Integrar principios éticos (justicia, privacidad, responsabilidad) desde el principio del desarrollo.\n            .col-lg-3.mb-4(data-aos=\"zoom-in-down\")\n              .custom-image-card-2.h-100\n                img.custom-image-card__image(src=\"@/assets/curso/tema2/25.png\" alt=\"\")\n                .custom-image-card__text.p-4\n                  h5.mb-2.text-center Transparencia y documentación\n                  p.mb-0.text-center Usar herramientas como <em>Model Cards y Datasheets for Datasets</em>, para documentar cómo se construyó el sistema y qué limitaciones tiene.\n            .col-lg-3.mb-4(data-aos=\"zoom-in-down\")\n              .custom-image-card-2.h-100\n                img.custom-image-card__image(src=\"@/assets/curso/tema2/26.png\" alt=\"\")\n                .custom-image-card__text.p-4\n                  h5.mb-2.text-center Cumplimiento legal\n                  p.mb-0.text-center Asegurar que el sistema cumple con normas como el Reglamento General de Protección de Datos (GDPR), leyes antidiscriminatorias, etc.\n          \n\n\n      .bg-full-width.border-top.color-primario\n        .p-4.p-md-5\n          h2(data-aos=\"fade-left\") MATERIAL COMPLEMENTARIO\n          .row.material-complementario\n            .col-12.col-md-6.col-lg-7\n              p Los invitamos a explorar el material complementario de este curso, en esta sección encontrará recursos que le permitirán profundizar  y enriquecer su aprendizaje en los temas tratados en esta unidad.\n  \n              p.d-flex.my-4\n                img.me-3(src='@/assets/componentes/link.svg' :style=\"{'max-width':'16px'}\")\n                a(href=\"https://elibro.net/es/lc/tecnologicadeloriente/titulos/117744\" target=\"_blank\" rel=\"noopener noreferrer\") Casas Roma, J. Nin Guerrero, J. & Julbe López, F. (2019). Big data: análisis de datos en entornos masivos: ( ed.). Editorial UOC. \n  \n              p.d-flex.my-4\n                img.me-3(src='@/assets/template/icono-yt.svg' :style=\"{'max-width':'16px'}\")\n                a(href=\"https://www.youtube.com/watch?v=ESLcIjf3Bs8\" target=\"_blank\" rel=\"noopener noreferrer\") NEWMEDIA UFM. (2025, 20 de marzo). Perspectiva ética y social de la IA\n  \n            .col-12.col-md-6.col-lg-3.offset-lg-1\n              figure\n                img(src='@/assets/componentes/material-complementario.svg', alt='')\n  \n  </template>\n\n<script>\nexport default {\n  name: 'Tema2',\n  mounted() {\n    this.$nextTick(() => {\n      this.$aosRefresh()\n    })\n  },\n}\n</script>\n\n<style lang=\"sass\"></style>\n","import mod from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--11-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Tema2.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../../node_modules/cache-loader/dist/cjs.js??ref--11-0!../../../node_modules/thread-loader/dist/cjs.js!../../../node_modules/babel-loader/lib/index.js!../../../node_modules/cache-loader/dist/cjs.js??ref--0-0!../../../node_modules/vue-loader/lib/index.js??vue-loader-options!./Tema2.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Tema2.vue?vue&type=template&id=616f79a6&lang=pug&\"\nimport script from \"./Tema2.vue?vue&type=script&lang=js&\"\nexport * from \"./Tema2.vue?vue&type=script&lang=js&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../../node_modules/vue-loader/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports","module.exports = __webpack_public_path__ + \"img/1.8b4c71ec.png\";","module.exports = __webpack_public_path__ + \"img/15.04cb2734.svg\";","module.exports = __webpack_public_path__ + \"img/slide-3.b6e5c4a0.png\";","module.exports = __webpack_public_path__ + \"img/info1.9a6caaf5.png\";","module.exports = __webpack_public_path__ + \"img/5.38f31e85.svg\";","module.exports = __webpack_public_path__ + \"img/7.4e9a7b66.png\";","module.exports = __webpack_public_path__ + \"img/6.faba8856.png\";","module.exports = __webpack_public_path__ + \"img/11.59edc4c2.png\";","module.exports = __webpack_public_path__ + \"img/13.b31fa50e.svg\";","module.exports = __webpack_public_path__ + \"img/link.317b045f.svg\";","module.exports = __webpack_public_path__ + \"img/8.d6079cd9.svg\";","module.exports = __webpack_public_path__ + \"img/20.7e14a536.svg\";","module.exports = __webpack_public_path__ + \"img/2.fcedb2ba.png\";","module.exports = __webpack_public_path__ + \"img/slide-1.920727a5.png\";","module.exports = __webpack_public_path__ + \"img/14.eb58f404.png\";","module.exports = __webpack_public_path__ + \"img/26.c09ee1bb.png\";","module.exports = __webpack_public_path__ + \"img/9.01eeebd6.png\";","module.exports = __webpack_public_path__ + \"img/4.470ed4cc.svg\";","module.exports = __webpack_public_path__ + \"img/21.5301cc58.svg\";","module.exports = __webpack_public_path__ + \"img/icono-pdf.5c464bfe.svg\";"],"sourceRoot":""}